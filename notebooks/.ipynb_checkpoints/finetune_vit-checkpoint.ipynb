{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83920511-c795-438a-ae24-6ed799130a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install accelerate transformers[torch] datasets \\\n",
    "#     pyav torchvision evaluate scikit-learn seaborn pandas \\\n",
    "#     albumentations opencv-python -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d021112e-d4d3-456c-a95f-81357a9c2b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, XCLIPVisionModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoImageProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from torchvision.transforms import RandomResizedCrop,  Compose, Normalize, ToTensor, Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc3ed172-b56c-4899-bf97-cd9f63891f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('TornikeO/imigue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0e3c381-1b8d-4660-a9e1-856e971b66a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=320x180>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=320x180>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=320x180>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=320x180>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=320x180>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=320x180>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=320x180>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=320x180>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=320x180>,\n",
       "  <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=320x180>],\n",
       " 'label': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'video': [10116,\n",
       "  10116,\n",
       "  10116,\n",
       "  10116,\n",
       "  10116,\n",
       "  10116,\n",
       "  10116,\n",
       "  10116,\n",
       "  10116,\n",
       "  10116],\n",
       " 'frame': [0, 11, 12, 13, 2, 3, 4, 5, 6, 7],\n",
       " 'id2label': ['Turtle neck',\n",
       "  'Turtle neck',\n",
       "  'Turtle neck',\n",
       "  'Turtle neck',\n",
       "  'Turtle neck',\n",
       "  'Turtle neck',\n",
       "  'Turtle neck',\n",
       "  'Turtle neck',\n",
       "  'Turtle neck',\n",
       "  'Turtle neck'],\n",
       " 'id2category': ['Head',\n",
       "  'Head',\n",
       "  'Head',\n",
       "  'Head',\n",
       "  'Head',\n",
       "  'Head',\n",
       "  'Head',\n",
       "  'Head',\n",
       "  'Head',\n",
       "  'Head'],\n",
       " 'split': ['train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train',\n",
       "  'train']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e4810-286b-48e6-9283-3ee45ad43431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Turtle neck', 1: 'Bulging face, deep breath', 2: 'Touching hat', 3: 'Touching or scratching head', 4: 'Touching or scratching forehead', 5: 'Covering face', 6: 'Rubbing eyes', 7: 'Touching or scratching facial parts', 8: 'Touching ears', 9: 'Biting nails', 10: 'Touching jaw', 11: 'Touching or scratching neck', 12: 'Playing with or adjusting hair', 13: 'Buckle button, pulling shirt collar, adjusting tie', 14: 'Touching or covering suprasternal notch', 15: 'Scratching back', 16: 'Folding arms', 17: 'Dustoffing clothes', 18: 'Putting arms behind body', 19: 'Moving torso', 20: 'Sitting upright', 21: 'Scratching or touching arms', 22: 'Rubbing or holding hands', 23: 'Crossing fingers', 24: 'Minaret gesture', 25: 'Playing with or manipulating objects', 26: 'Hold back arms', 27: 'Head up', 28: 'Pressing lips', 29: 'Arms akimbo', 30: 'Shaking shoulders', 31: 'Illustrative body language'}\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "\n",
    "train_transforms = T.Compose([\n",
    "    T.Resize(size=size),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomAffine(degrees=5,\n",
    "                   translate=(0.01, 0.03),\n",
    "                   scale=(.95, 1.05),),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize(size=size),\n",
    "    T.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "def preprocess_train(examples):\n",
    "    # print(examples)\n",
    "    examples[\"pixel_values\"] = [\n",
    "        train_transforms(image) for image in examples[\"image\"]\n",
    "    ]\n",
    "    examples['labels'] = [l-1 for l in examples['label']]\n",
    "    del examples['image']\n",
    "    del examples['label']\n",
    "    return examples\n",
    "\n",
    "def preprocess_val(examples):\n",
    "    # print(examples)\n",
    "    examples[\"pixel_values\"] = [\n",
    "        val_transforms(image) for image in examples[\"image\"]\n",
    "    ]\n",
    "    examples['labels'] = [l-1 for l in examples['label']]\n",
    "    \n",
    "    del examples['image']\n",
    "    del examples['label']\n",
    "    return examples\n",
    "\n",
    "names = pd.read_csv('names.csv')\n",
    "names.id = names.index\n",
    "label2id = dict(zip(names.id2label, names.id))\n",
    "id2label = dict(zip(names.id, names.id2label))\n",
    "print(id2label)\n",
    "\n",
    "train_ds = data['train'].shuffle(42)\n",
    "val_ds = data['test']\n",
    "train_ds.set_transform(preprocess_train, columns=['image', 'label'], output_all_columns=False)\n",
    "val_ds.set_transform(preprocess_val, columns=['image', 'label'], output_all_columns=False)\n",
    "\n",
    "fig, axs = plt.subplots(4, 5, figsize=(20, 10))\n",
    "axs = axs.ravel()\n",
    "for i, (d, ax) in enumerate(zip(train_ds, axs.ravel())):\n",
    "    if i == 0:\n",
    "        print(d['pixel_values'].shape)\n",
    "    ax.imshow((d['pixel_values'].permute(1,2,0) + .7).clip(0,1))\n",
    "    ax.set_title(id2label[d['labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "775dbb8e-3e31-49cc-800b-ce3cd4d83350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fa778fe-663d-4d93-966e-3d851f36d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b438022f-b40e-4be9-9467-14a21cc277eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Turtle neck',\n",
       " 1: 'Bulging face, deep breath',\n",
       " 2: 'Touching hat',\n",
       " 3: 'Touching or scratching head',\n",
       " 4: 'Touching or scratching forehead',\n",
       " 5: 'Covering face',\n",
       " 6: 'Rubbing eyes',\n",
       " 7: 'Touching or scratching facial parts',\n",
       " 8: 'Touching ears',\n",
       " 9: 'Biting nails',\n",
       " 10: 'Touching jaw',\n",
       " 11: 'Touching or scratching neck',\n",
       " 12: 'Playing with or adjusting hair',\n",
       " 13: 'Buckle button, pulling shirt collar, adjusting tie',\n",
       " 14: 'Touching or covering suprasternal notch',\n",
       " 15: 'Scratching back',\n",
       " 16: 'Folding arms',\n",
       " 17: 'Dustoffing clothes',\n",
       " 18: 'Putting arms behind body',\n",
       " 19: 'Moving torso',\n",
       " 20: 'Sitting upright',\n",
       " 21: 'Scratching or touching arms',\n",
       " 22: 'Rubbing or holding hands',\n",
       " 23: 'Crossing fingers',\n",
       " 24: 'Minaret gesture',\n",
       " 25: 'Playing with or manipulating objects',\n",
       " 26: 'Hold back arms',\n",
       " 27: 'Head up',\n",
       " 28: 'Pressing lips',\n",
       " 29: 'Arms akimbo',\n",
       " 30: 'Shaking shoulders',\n",
       " 31: 'Illustrative body language'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c95c40ed-55b3-4dc8-83d8-436041145ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('names.csv')\n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b9387d9-03e3-4a2b-9224-5af23d695522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    # 'google/vit-base-patch16-224'\n",
    "    num_labels=len(id2label),\n",
    "    # id2label=id2label,\n",
    "    # label2id=label2id,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d6986e1-ffc4-4414-93a6-7687e09560fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 29\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpredictions, references\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m      9\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     10\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     remove_unused_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     metric_for_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:368\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfull_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_utils.py:95\u001b[0m, in \u001b[0;36mset_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     93\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m---> 95\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/random.py:40\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/random.py:126\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    123\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    124\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 126\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:232\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 232\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/random.py:124\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    123\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 124\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "! rm -rf model\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # return f1_metric.compute(predictions=predictions, references=labels)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "  \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=64,\n",
    "    dataloader_num_workers=24,\n",
    "    num_train_epochs=1,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    # seed=42,\n",
    "    # fp16=False,\n",
    "    # use_cpu=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c5c60-eb62-460a-880c-609ea97c5872",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba502b6-7b24-441d-b8ce-39be2ae881a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"image-classification\", model='model/checkpoint-756', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4bc932-7fb0-42fe-b66f-dd0e0aa254da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('TornikeO/imigue',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea54264-076d-4d59-9363-f207733464ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "for i in range(0, 10_000, 500):\n",
    "    sample = data['train'][i]\n",
    "    sample_img = data['train'][i]['image']\n",
    "    sample_class = data['train'][i]['class'] - 1 # Don't forget this!\n",
    "    sample_class = id2label[sample_class]\n",
    "    print('Actual class ',  sample_class)\n",
    "    display(sample_img)\n",
    "    pred = classifier(sample_img)\n",
    "    # Predictions are pretty crap!\n",
    "    print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
