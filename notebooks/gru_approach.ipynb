{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "from GRU import BIGRU\n",
    "import pytorch_utils\n",
    "\n",
    "import torch\n",
    "torch.zeros(1).cuda()\n",
    "\n",
    "dataset_dir = 'training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper functions for visualization\n",
    "\n",
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'black',\n",
    "    (7, 9): 'black',\n",
    "    (6, 8): 'white',\n",
    "    (8, 10): 'white',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"movenet_thunder\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
    "\n",
    "def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "    input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "    A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "    coordinates and scores.\n",
    "    \"\"\"\n",
    "\n",
    "    if \"movenet_lightning\" in model_name:\n",
    "        module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "        input_size = 192\n",
    "    elif \"movenet_thunder\" in model_name:\n",
    "        module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "        input_size = 256\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints_with_scores = outputs['output_0'].numpy()\n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(path):\n",
    "    # Load the input image.\n",
    "    image_path = os.path.join(dir, path)#'training/16/11783.3.jpg'\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "\n",
    "    # Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "    # Run model inference.\n",
    "    keypoints_with_scores = movenet(input_image)\n",
    "    print(keypoints_with_scores)\n",
    "\n",
    "    # Visualize the predictions with image.\n",
    "    display_image = tf.expand_dims(image, axis=0)\n",
    "    display_image = tf.cast(tf.image.resize_with_pad(\n",
    "        display_image, 1280, 1280), dtype=tf.int32)\n",
    "    output_overlay = draw_prediction_on_image(\n",
    "        np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores, threshold=0.3)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(output_overlay)\n",
    "    _ = plt.axis('off')\n",
    "\n",
    "# import os\n",
    "# from ipywidgets import interact\n",
    "# dir = 'training/17'\n",
    "# files = os.listdir(dir)\n",
    "# interact(process, path=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imigue(dir):\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    df = pd.DataFrame(columns=['path', 'class', 'video_id', 'frame'])\n",
    "    for i in range(1, 33):\n",
    "        dir = os.path.join(dir, str(i))\n",
    "        files = os.listdir(dir)\n",
    "        for file in files:\n",
    "            df.loc[len(df.index)] = [os.path.join(str(i), file), i, file.split('.')[0], file.split('.')[1]]\n",
    "    return df\n",
    "    # return pd.read_csv('metadata.csv')\n",
    "\n",
    "df = load_imigue(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pose_data(df):\n",
    "    for k in KEYPOINT_DICT.keys():\n",
    "        df[f'{k}_0'] = 0.0\n",
    "        df[f'{k}_1'] = 0.0\n",
    "        df[f'{k}_score'] = 0.0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        # Load the input image.\n",
    "        image_path = os.path.join(dataset_dir, row.path)\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_jpeg(image)\n",
    "\n",
    "        # Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "        input_size = 256\n",
    "        input_image = tf.expand_dims(image, axis=0)\n",
    "        input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "        # Run model inference.\n",
    "        keypoints_with_scores = movenet(input_image)\n",
    "        for k, v in KEYPOINT_DICT.items():\n",
    "            values = keypoints_with_scores[0][0][v]\n",
    "            df.iloc[i, df.columns.get_loc(f'{k}_0')] = values[0]\n",
    "            df.iloc[i, df.columns.get_loc(f'{k}_1')] = values[1]\n",
    "            df.iloc[i, df.columns.get_loc(f'{k}_score')] = values[2]\n",
    "\n",
    "add_pose_data(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_input(df):\n",
    "    df = df[['class', 'video_id', 'nose_0', 'nose_1', 'nose_score',\n",
    "       'left_eye_0', 'left_eye_1', 'left_eye_score', 'right_eye_0',\n",
    "       'right_eye_1', 'right_eye_score', 'left_ear_0', 'left_ear_1',\n",
    "       'left_ear_score', 'right_ear_0', 'right_ear_1', 'right_ear_score',\n",
    "       'left_shoulder_0', 'left_shoulder_1', 'left_shoulder_score',\n",
    "       'right_shoulder_0', 'right_shoulder_1', 'right_shoulder_score',\n",
    "       'left_elbow_0', 'left_elbow_1', 'left_elbow_score', 'right_elbow_0',\n",
    "       'right_elbow_1', 'right_elbow_score', 'left_wrist_0', 'left_wrist_1',\n",
    "       'left_wrist_score']]\n",
    "    df = df.groupby(['class','video_id']).apply(lambda x: x.values.tolist()).tolist()\n",
    "    seq_target_count = 7\n",
    "    data_tmp = []\n",
    "    for i in range(len(df)):\n",
    "        sequences = df[i]\n",
    "        data_tmp.append([])\n",
    "        for j in range(seq_target_count):\n",
    "            if j < len(sequences):\n",
    "                data_tmp[i].append(sequences[j])\n",
    "            else:\n",
    "                # data_tmp[i].append(sequences[-1])\n",
    "                data_tmp[i].append([0] * 32)\n",
    "    data_tmp = np.array(data_tmp)\n",
    "    # data_tmp = data_tmp[:11000]\n",
    "    return data_tmp[:, :, 2:], data_tmp[:, 0, 0].flatten().astype(int)\n",
    "\n",
    "data_X, data_Y = make_lstm_input(df)\n",
    "data_Y = data_Y - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = BIGRU(30, 50, num_class=32, device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "for i in range(10):\n",
    "    loss = pytorch_utils.train_one_epoch(i, model, pytorch_utils.make_dataset(X_train, y_train, batch_size=64), optimizer, loss_fn, device=device)\n",
    "# torch.save(model, 'bigru.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(torch.Tensor(X_test).to(device))\n",
    "    maxk = 1\n",
    "    _, y_pred = output.topk(maxk, 1, True, True)\n",
    "    y_pred = y_pred.t()[0].cpu().numpy()\n",
    "\n",
    "# y_pred = model(X_test)\n",
    "# y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, include_values=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
