{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '..')\n",
    "from GRU import BIGRU\n",
    "import pytorch_utils\n",
    "\n",
    "import torch\n",
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper functions for visualization\n",
    "\n",
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'black',\n",
    "    (7, 9): 'black',\n",
    "    (6, 8): 'white',\n",
    "    (8, 10): 'white',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.11):\n",
    "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "  Args:\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "      visualized.\n",
    "\n",
    "  Returns:\n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "      * the coordinates of all keypoints of all detected entities;\n",
    "      * the coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "  \"\"\"\n",
    "  keypoints_all = []\n",
    "  keypoint_edges_all = []\n",
    "  edge_colors = []\n",
    "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "  for idx in range(num_instances):\n",
    "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "        kpts_scores > keypoint_threshold, :]\n",
    "    keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "        keypoint_edges_all.append(line_seg)\n",
    "        edge_colors.append(color)\n",
    "  if keypoints_all:\n",
    "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "  else:\n",
    "    keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "  if keypoint_edges_all:\n",
    "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "  else:\n",
    "    edges_xy = np.zeros((0, 2, 2))\n",
    "  return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None, threshold=.11):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "  Args:\n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "      of the crop region in normalized coordinates (see the init_crop_region\n",
    "      function below for more detail). If provided, this function will also\n",
    "      draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "      Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  # To remove the huge white borders\n",
    "  fig.tight_layout(pad=0)\n",
    "  ax.margins(0)\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "  plt.axis('off')\n",
    "\n",
    "  im = ax.imshow(image)\n",
    "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "  ax.add_collection(line_segments)\n",
    "  # Turn off tick labels\n",
    "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "  (keypoint_locs, keypoint_edges,\n",
    "   edge_colors) = _keypoints_and_edges_for_display(\n",
    "       keypoints_with_scores, height, width, keypoint_threshold=threshold)\n",
    "\n",
    "  line_segments.set_segments(keypoint_edges)\n",
    "  line_segments.set_color(edge_colors)\n",
    "  if keypoint_edges.shape[0]:\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "  if keypoint_locs.shape[0]:\n",
    "    scat.set_offsets(keypoint_locs)\n",
    "\n",
    "  if crop_region is not None:\n",
    "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "    rect = patches.Rectangle(\n",
    "        (xmin,ymin),rec_width,rec_height,\n",
    "        linewidth=1,edgecolor='b',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "  fig.canvas.draw()\n",
    "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "  image_from_plot = image_from_plot.reshape(\n",
    "      fig.canvas.get_width_height()[::-1] + (3,))\n",
    "  plt.close(fig)\n",
    "  if output_image_height is not None:\n",
    "    output_image_width = int(output_image_height / height * width)\n",
    "    image_from_plot = cv2.resize(\n",
    "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "         interpolation=cv2.INTER_CUBIC)\n",
    "  return image_from_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"movenet_thunder\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
    "\n",
    "def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "    input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "    A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "    coordinates and scores.\n",
    "    \"\"\"\n",
    "\n",
    "    if \"movenet_lightning\" in model_name:\n",
    "        module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "        input_size = 192\n",
    "    elif \"movenet_thunder\" in model_name:\n",
    "        module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "        input_size = 256\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints_with_scores = outputs['output_0'].numpy()\n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(path):\n",
    "    # Load the input image.\n",
    "    image_path = os.path.join(dir, path)#'training/16/11783.3.jpg'\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "\n",
    "    # Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "    # Run model inference.\n",
    "    keypoints_with_scores = movenet(input_image)\n",
    "    print(keypoints_with_scores)\n",
    "\n",
    "    # Visualize the predictions with image.\n",
    "    display_image = tf.expand_dims(image, axis=0)\n",
    "    display_image = tf.cast(tf.image.resize_with_pad(\n",
    "        display_image, 1280, 1280), dtype=tf.int32)\n",
    "    output_overlay = draw_prediction_on_image(\n",
    "        np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores, threshold=0.3)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(output_overlay)\n",
    "    _ = plt.axis('off')\n",
    "\n",
    "# import os\n",
    "# from ipywidgets import interact\n",
    "# dir = 'training/17'\n",
    "# files = os.listdir(dir)\n",
    "# interact(process, path=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imigue():\n",
    "    import pandas as pd\n",
    "    # import os\n",
    "\n",
    "    # df = pd.DataFrame(columns=['path', 'class', 'video_id', 'frame'])\n",
    "    # for i in range(1, 33):\n",
    "    #     dir = 'training/' + str(i)\n",
    "    #     files = os.listdir(dir)\n",
    "    #     for file in files:\n",
    "    #         df.loc[len(df.index)] = [os.path.join(str(i), file), i, file.split('.')[0], file.split('.')[1]]\n",
    "    # return df\n",
    "    return pd.read_csv('metadata.csv')\n",
    "\n",
    "df = load_imigue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>path</th>\n",
       "      <th>class</th>\n",
       "      <th>video_id</th>\n",
       "      <th>frame</th>\n",
       "      <th>nose_0</th>\n",
       "      <th>nose_1</th>\n",
       "      <th>nose_score</th>\n",
       "      <th>left_eye_0</th>\n",
       "      <th>left_eye_1</th>\n",
       "      <th>...</th>\n",
       "      <th>left_knee_score</th>\n",
       "      <th>right_knee_0</th>\n",
       "      <th>right_knee_1</th>\n",
       "      <th>right_knee_score</th>\n",
       "      <th>left_ankle_0</th>\n",
       "      <th>left_ankle_1</th>\n",
       "      <th>left_ankle_score</th>\n",
       "      <th>right_ankle_0</th>\n",
       "      <th>right_ankle_1</th>\n",
       "      <th>right_ankle_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>1/676.2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>676</td>\n",
       "      <td>2</td>\n",
       "      <td>0.536940</td>\n",
       "      <td>0.519220</td>\n",
       "      <td>0.176915</td>\n",
       "      <td>0.482901</td>\n",
       "      <td>0.554157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127321</td>\n",
       "      <td>0.739197</td>\n",
       "      <td>0.322254</td>\n",
       "      <td>0.401741</td>\n",
       "      <td>0.723482</td>\n",
       "      <td>0.439374</td>\n",
       "      <td>0.097672</td>\n",
       "      <td>0.753613</td>\n",
       "      <td>0.624171</td>\n",
       "      <td>0.215106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>1/676.3.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>676</td>\n",
       "      <td>3</td>\n",
       "      <td>0.527566</td>\n",
       "      <td>0.505588</td>\n",
       "      <td>0.320876</td>\n",
       "      <td>0.486930</td>\n",
       "      <td>0.538982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112204</td>\n",
       "      <td>0.738475</td>\n",
       "      <td>0.322620</td>\n",
       "      <td>0.354696</td>\n",
       "      <td>0.742008</td>\n",
       "      <td>0.587786</td>\n",
       "      <td>0.084846</td>\n",
       "      <td>0.752234</td>\n",
       "      <td>0.625011</td>\n",
       "      <td>0.210873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>1/676.4.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>676</td>\n",
       "      <td>4</td>\n",
       "      <td>0.520691</td>\n",
       "      <td>0.523298</td>\n",
       "      <td>0.207465</td>\n",
       "      <td>0.473461</td>\n",
       "      <td>0.559082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149931</td>\n",
       "      <td>0.735181</td>\n",
       "      <td>0.318123</td>\n",
       "      <td>0.483617</td>\n",
       "      <td>0.738399</td>\n",
       "      <td>0.582698</td>\n",
       "      <td>0.114686</td>\n",
       "      <td>0.751253</td>\n",
       "      <td>0.611784</td>\n",
       "      <td>0.219790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90</td>\n",
       "      <td>1/676.5.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>676</td>\n",
       "      <td>5</td>\n",
       "      <td>0.541147</td>\n",
       "      <td>0.514972</td>\n",
       "      <td>0.211949</td>\n",
       "      <td>0.487922</td>\n",
       "      <td>0.551379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116105</td>\n",
       "      <td>0.735202</td>\n",
       "      <td>0.317601</td>\n",
       "      <td>0.381059</td>\n",
       "      <td>0.737782</td>\n",
       "      <td>0.582862</td>\n",
       "      <td>0.094748</td>\n",
       "      <td>0.752435</td>\n",
       "      <td>0.624138</td>\n",
       "      <td>0.167352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>1/676.6.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>676</td>\n",
       "      <td>6</td>\n",
       "      <td>0.508418</td>\n",
       "      <td>0.529349</td>\n",
       "      <td>0.218647</td>\n",
       "      <td>0.461819</td>\n",
       "      <td>0.569866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145684</td>\n",
       "      <td>0.739282</td>\n",
       "      <td>0.318815</td>\n",
       "      <td>0.468959</td>\n",
       "      <td>0.708210</td>\n",
       "      <td>0.466800</td>\n",
       "      <td>0.120853</td>\n",
       "      <td>0.745579</td>\n",
       "      <td>0.605821</td>\n",
       "      <td>0.268091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60509</th>\n",
       "      <td>56970</td>\n",
       "      <td>32/17699.5.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>17699</td>\n",
       "      <td>5</td>\n",
       "      <td>0.410697</td>\n",
       "      <td>0.550067</td>\n",
       "      <td>0.300542</td>\n",
       "      <td>0.371683</td>\n",
       "      <td>0.582072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059571</td>\n",
       "      <td>0.774956</td>\n",
       "      <td>0.343825</td>\n",
       "      <td>0.112110</td>\n",
       "      <td>0.775187</td>\n",
       "      <td>0.539377</td>\n",
       "      <td>0.033483</td>\n",
       "      <td>0.778112</td>\n",
       "      <td>0.508907</td>\n",
       "      <td>0.057826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60510</th>\n",
       "      <td>56615</td>\n",
       "      <td>32/17707.0.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>17707</td>\n",
       "      <td>0</td>\n",
       "      <td>0.497047</td>\n",
       "      <td>0.554310</td>\n",
       "      <td>0.278072</td>\n",
       "      <td>0.464215</td>\n",
       "      <td>0.599753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069238</td>\n",
       "      <td>0.768674</td>\n",
       "      <td>0.450558</td>\n",
       "      <td>0.154153</td>\n",
       "      <td>0.779981</td>\n",
       "      <td>0.601488</td>\n",
       "      <td>0.026040</td>\n",
       "      <td>0.771737</td>\n",
       "      <td>0.491129</td>\n",
       "      <td>0.176495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60511</th>\n",
       "      <td>54972</td>\n",
       "      <td>32/17707.1.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>17707</td>\n",
       "      <td>1</td>\n",
       "      <td>0.456710</td>\n",
       "      <td>0.519351</td>\n",
       "      <td>0.246757</td>\n",
       "      <td>0.391514</td>\n",
       "      <td>0.559125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090489</td>\n",
       "      <td>0.772732</td>\n",
       "      <td>0.323692</td>\n",
       "      <td>0.090784</td>\n",
       "      <td>0.769023</td>\n",
       "      <td>0.533433</td>\n",
       "      <td>0.116141</td>\n",
       "      <td>0.775654</td>\n",
       "      <td>0.447653</td>\n",
       "      <td>0.127975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60512</th>\n",
       "      <td>54629</td>\n",
       "      <td>32/17707.2.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>17707</td>\n",
       "      <td>2</td>\n",
       "      <td>0.439786</td>\n",
       "      <td>0.550717</td>\n",
       "      <td>0.238725</td>\n",
       "      <td>0.387315</td>\n",
       "      <td>0.580766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037351</td>\n",
       "      <td>0.772637</td>\n",
       "      <td>0.379241</td>\n",
       "      <td>0.069503</td>\n",
       "      <td>0.772449</td>\n",
       "      <td>0.558960</td>\n",
       "      <td>0.052951</td>\n",
       "      <td>0.778140</td>\n",
       "      <td>0.529624</td>\n",
       "      <td>0.055858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60513</th>\n",
       "      <td>55886</td>\n",
       "      <td>32/17707.3.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>17707</td>\n",
       "      <td>3</td>\n",
       "      <td>0.464288</td>\n",
       "      <td>0.490305</td>\n",
       "      <td>0.316756</td>\n",
       "      <td>0.409161</td>\n",
       "      <td>0.523831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068950</td>\n",
       "      <td>0.604666</td>\n",
       "      <td>0.316377</td>\n",
       "      <td>0.240594</td>\n",
       "      <td>0.759623</td>\n",
       "      <td>0.509146</td>\n",
       "      <td>0.161589</td>\n",
       "      <td>0.765042</td>\n",
       "      <td>0.526195</td>\n",
       "      <td>0.229589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60514 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0            path  class  video_id  frame    nose_0    nose_1  \\\n",
       "0              41     1/676.2.jpg      1       676      2  0.536940  0.519220   \n",
       "1              71     1/676.3.jpg      1       676      3  0.527566  0.505588   \n",
       "2              19     1/676.4.jpg      1       676      4  0.520691  0.523298   \n",
       "3              90     1/676.5.jpg      1       676      5  0.541147  0.514972   \n",
       "4              43     1/676.6.jpg      1       676      6  0.508418  0.529349   \n",
       "...           ...             ...    ...       ...    ...       ...       ...   \n",
       "60509       56970  32/17699.5.jpg     32     17699      5  0.410697  0.550067   \n",
       "60510       56615  32/17707.0.jpg     32     17707      0  0.497047  0.554310   \n",
       "60511       54972  32/17707.1.jpg     32     17707      1  0.456710  0.519351   \n",
       "60512       54629  32/17707.2.jpg     32     17707      2  0.439786  0.550717   \n",
       "60513       55886  32/17707.3.jpg     32     17707      3  0.464288  0.490305   \n",
       "\n",
       "       nose_score  left_eye_0  left_eye_1  ...  left_knee_score  right_knee_0  \\\n",
       "0        0.176915    0.482901    0.554157  ...         0.127321      0.739197   \n",
       "1        0.320876    0.486930    0.538982  ...         0.112204      0.738475   \n",
       "2        0.207465    0.473461    0.559082  ...         0.149931      0.735181   \n",
       "3        0.211949    0.487922    0.551379  ...         0.116105      0.735202   \n",
       "4        0.218647    0.461819    0.569866  ...         0.145684      0.739282   \n",
       "...           ...         ...         ...  ...              ...           ...   \n",
       "60509    0.300542    0.371683    0.582072  ...         0.059571      0.774956   \n",
       "60510    0.278072    0.464215    0.599753  ...         0.069238      0.768674   \n",
       "60511    0.246757    0.391514    0.559125  ...         0.090489      0.772732   \n",
       "60512    0.238725    0.387315    0.580766  ...         0.037351      0.772637   \n",
       "60513    0.316756    0.409161    0.523831  ...         0.068950      0.604666   \n",
       "\n",
       "       right_knee_1  right_knee_score  left_ankle_0  left_ankle_1  \\\n",
       "0          0.322254          0.401741      0.723482      0.439374   \n",
       "1          0.322620          0.354696      0.742008      0.587786   \n",
       "2          0.318123          0.483617      0.738399      0.582698   \n",
       "3          0.317601          0.381059      0.737782      0.582862   \n",
       "4          0.318815          0.468959      0.708210      0.466800   \n",
       "...             ...               ...           ...           ...   \n",
       "60509      0.343825          0.112110      0.775187      0.539377   \n",
       "60510      0.450558          0.154153      0.779981      0.601488   \n",
       "60511      0.323692          0.090784      0.769023      0.533433   \n",
       "60512      0.379241          0.069503      0.772449      0.558960   \n",
       "60513      0.316377          0.240594      0.759623      0.509146   \n",
       "\n",
       "       left_ankle_score  right_ankle_0  right_ankle_1  right_ankle_score  \n",
       "0              0.097672       0.753613       0.624171           0.215106  \n",
       "1              0.084846       0.752234       0.625011           0.210873  \n",
       "2              0.114686       0.751253       0.611784           0.219790  \n",
       "3              0.094748       0.752435       0.624138           0.167352  \n",
       "4              0.120853       0.745579       0.605821           0.268091  \n",
       "...                 ...            ...            ...                ...  \n",
       "60509          0.033483       0.778112       0.508907           0.057826  \n",
       "60510          0.026040       0.771737       0.491129           0.176495  \n",
       "60511          0.116141       0.775654       0.447653           0.127975  \n",
       "60512          0.052951       0.778140       0.529624           0.055858  \n",
       "60513          0.161589       0.765042       0.526195           0.229589  \n",
       "\n",
       "[60514 rows x 56 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_pose_data(df):\n",
    "    for k in KEYPOINT_DICT.keys():\n",
    "        df[f'{k}_0'] = 0.0\n",
    "        df[f'{k}_1'] = 0.0\n",
    "        df[f'{k}_score'] = 0.0\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        row = df.iloc[i]\n",
    "        # Load the input image.\n",
    "        image_path = os.path.join('training', row.path)\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_jpeg(image)\n",
    "\n",
    "        # Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "        input_size = 256\n",
    "        input_image = tf.expand_dims(image, axis=0)\n",
    "        input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "        # Run model inference.\n",
    "        keypoints_with_scores = movenet(input_image)\n",
    "        for k, v in KEYPOINT_DICT.items():\n",
    "            values = keypoints_with_scores[0][0][v]\n",
    "            df.iloc[i, df.columns.get_loc(f'{k}_0')] = values[0]\n",
    "            df.iloc[i, df.columns.get_loc(f'{k}_1')] = values[1]\n",
    "            df.iloc[i, df.columns.get_loc(f'{k}_score')] = values[2]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lstm_input(df):\n",
    "    df = df[['class', 'video_id', 'nose_0', 'nose_1', 'nose_score',\n",
    "       'left_eye_0', 'left_eye_1', 'left_eye_score', 'right_eye_0',\n",
    "       'right_eye_1', 'right_eye_score', 'left_ear_0', 'left_ear_1',\n",
    "       'left_ear_score', 'right_ear_0', 'right_ear_1', 'right_ear_score',\n",
    "       'left_shoulder_0', 'left_shoulder_1', 'left_shoulder_score',\n",
    "       'right_shoulder_0', 'right_shoulder_1', 'right_shoulder_score',\n",
    "       'left_elbow_0', 'left_elbow_1', 'left_elbow_score', 'right_elbow_0',\n",
    "       'right_elbow_1', 'right_elbow_score', 'left_wrist_0', 'left_wrist_1',\n",
    "       'left_wrist_score']]\n",
    "    df = df.groupby(['class','video_id']).apply(lambda x: x.values.tolist()).tolist()\n",
    "    seq_target_count = 7\n",
    "    data_tmp = []\n",
    "    for i in range(len(df)):\n",
    "        sequences = df[i]\n",
    "        data_tmp.append([])\n",
    "        for j in range(seq_target_count):\n",
    "            if j < len(sequences):\n",
    "                data_tmp[i].append(sequences[j])\n",
    "            else:\n",
    "                # data_tmp[i].append(sequences[-1])\n",
    "                data_tmp[i].append([0] * 32)\n",
    "    data_tmp = np.array(data_tmp)\n",
    "    data_tmp = data_tmp[:11000]\n",
    "    return data_tmp[:, :, 2:], data_tmp[:, 0, 0].flatten().astype(int)\n",
    "\n",
    "# data_x = make_lstm_input(df)\n",
    "# data[class_video][data_seq_index][32]\n",
    "data_X, data_Y = make_lstm_input(df)\n",
    "data_Y = data_Y - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for sequence classification in the IMDB dataset\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.datasets import imdb\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.layers import LSTM, Conv2D, MaxPooling2D\n",
    "# from tensorflow.keras.layers import Embedding\n",
    "# from tensorflow.keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "# tf.random.set_seed(7)\n",
    "# create the model\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(100, input_shape=(50, 30)))\n",
    "# model.add(Dense(32, activation='softmax'))\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "# model.fit(X_train, y_train, epochs=1, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 1 loss: 3.4851810932159424\n",
      "  batch 2 loss: 3.4366769790649414\n",
      "  batch 3 loss: 3.3808281421661377\n",
      "  batch 4 loss: 3.337273120880127\n",
      "  batch 5 loss: 3.2922539710998535\n",
      "  batch 6 loss: 3.286862373352051\n",
      "  batch 7 loss: 3.1937873363494873\n",
      "  batch 8 loss: 3.077843427658081\n",
      "  batch 9 loss: 3.0385890007019043\n",
      "  batch 10 loss: 3.00408673286438\n",
      "  batch 11 loss: 3.0159494876861572\n",
      "  batch 12 loss: 2.999701499938965\n",
      "  batch 13 loss: 2.7429139614105225\n",
      "  batch 14 loss: 2.7997617721557617\n",
      "  batch 15 loss: 3.0101804733276367\n",
      "  batch 16 loss: 2.723828077316284\n",
      "  batch 17 loss: 2.475606918334961\n",
      "  batch 18 loss: 2.774893045425415\n",
      "  batch 19 loss: 2.718452215194702\n",
      "  batch 20 loss: 2.502481460571289\n",
      "  batch 21 loss: 2.6398355960845947\n",
      "  batch 22 loss: 2.7291083335876465\n",
      "  batch 23 loss: 2.6569719314575195\n",
      "  batch 24 loss: 2.812772035598755\n",
      "  batch 25 loss: 2.994558095932007\n",
      "  batch 26 loss: 2.825977087020874\n",
      "  batch 27 loss: 2.793628692626953\n",
      "  batch 28 loss: 2.3324475288391113\n",
      "  batch 29 loss: 2.6905922889709473\n",
      "  batch 30 loss: 2.73960280418396\n",
      "  batch 31 loss: 2.619560956954956\n",
      "  batch 32 loss: 2.6962924003601074\n",
      "  batch 33 loss: 2.867138385772705\n",
      "  batch 34 loss: 2.532109498977661\n",
      "  batch 35 loss: 2.489506959915161\n",
      "  batch 36 loss: 2.414827823638916\n",
      "  batch 37 loss: 2.3422887325286865\n",
      "  batch 38 loss: 2.553342580795288\n",
      "  batch 39 loss: 2.6885886192321777\n",
      "  batch 40 loss: 2.5171926021575928\n",
      "  batch 41 loss: 2.667921543121338\n",
      "  batch 42 loss: 2.52543568611145\n",
      "  batch 43 loss: 2.3081166744232178\n",
      "  batch 44 loss: 2.5933382511138916\n",
      "  batch 45 loss: 2.555523633956909\n",
      "  batch 46 loss: 2.6665055751800537\n",
      "  batch 47 loss: 2.5861330032348633\n",
      "  batch 48 loss: 2.417557954788208\n",
      "  batch 49 loss: 2.4741597175598145\n",
      "  batch 50 loss: 2.497697353363037\n",
      "  batch 51 loss: 3.0461935997009277\n",
      "  batch 52 loss: 2.7022783756256104\n",
      "  batch 53 loss: 2.6773180961608887\n",
      "  batch 54 loss: 2.38018536567688\n",
      "  batch 55 loss: 2.78121018409729\n",
      "  batch 56 loss: 2.7089972496032715\n",
      "  batch 57 loss: 2.614640712738037\n",
      "  batch 58 loss: 2.4660556316375732\n",
      "  batch 59 loss: 2.527416467666626\n",
      "  batch 60 loss: 2.671809434890747\n",
      "  batch 61 loss: 2.5843982696533203\n",
      "  batch 62 loss: 2.563027858734131\n",
      "  batch 63 loss: 2.5290298461914062\n",
      "  batch 64 loss: 2.6700382232666016\n",
      "  batch 65 loss: 2.6782262325286865\n",
      "  batch 66 loss: 2.51938533782959\n",
      "  batch 67 loss: 2.322969913482666\n",
      "  batch 68 loss: 2.664633274078369\n",
      "  batch 69 loss: 2.428910970687866\n",
      "  batch 70 loss: 2.6844534873962402\n",
      "  batch 71 loss: 2.593750238418579\n",
      "  batch 72 loss: 2.748323678970337\n",
      "  batch 73 loss: 2.3223674297332764\n",
      "  batch 74 loss: 2.511281967163086\n",
      "  batch 75 loss: 2.3994622230529785\n",
      "  batch 76 loss: 2.480090379714966\n",
      "  batch 77 loss: 2.7347006797790527\n",
      "  batch 78 loss: 2.6210646629333496\n",
      "  batch 79 loss: 2.54947829246521\n",
      "  batch 80 loss: 2.632371187210083\n",
      "  batch 81 loss: 2.5595951080322266\n",
      "  batch 82 loss: 2.375730037689209\n",
      "  batch 83 loss: 2.5139267444610596\n",
      "  batch 84 loss: 2.4162421226501465\n",
      "  batch 85 loss: 2.3259427547454834\n",
      "  batch 86 loss: 2.4369301795959473\n",
      "  batch 87 loss: 2.4015748500823975\n",
      "  batch 88 loss: 2.3318934440612793\n",
      "  batch 89 loss: 2.7335221767425537\n",
      "  batch 90 loss: 2.666243076324463\n",
      "  batch 91 loss: 2.6971001625061035\n",
      "  batch 92 loss: 2.5373482704162598\n",
      "  batch 93 loss: 2.51513671875\n",
      "  batch 94 loss: 2.6384315490722656\n",
      "  batch 95 loss: 2.3095803260803223\n",
      "  batch 96 loss: 2.43894362449646\n",
      "  batch 97 loss: 2.6221349239349365\n",
      "  batch 98 loss: 2.5950021743774414\n",
      "  batch 99 loss: 2.3534252643585205\n",
      "  batch 100 loss: 2.9067838191986084\n",
      "  batch 101 loss: 2.466002941131592\n",
      "  batch 102 loss: 2.4314186573028564\n",
      "  batch 103 loss: 2.669640064239502\n",
      "  batch 104 loss: 2.4768497943878174\n",
      "  batch 105 loss: 2.526005268096924\n",
      "  batch 106 loss: 2.541303873062134\n",
      "  batch 107 loss: 2.6030757427215576\n",
      "  batch 108 loss: 2.6258740425109863\n",
      "  batch 109 loss: 2.567944288253784\n",
      "  batch 110 loss: 2.4841864109039307\n",
      "  batch 111 loss: 2.411280632019043\n",
      "  batch 112 loss: 2.4831855297088623\n",
      "  batch 113 loss: 2.468505382537842\n",
      "  batch 114 loss: 2.5400495529174805\n",
      "  batch 115 loss: 2.5779638290405273\n",
      "  batch 116 loss: 3.055356025695801\n",
      "  batch 1 loss: 2.3478453159332275\n",
      "  batch 2 loss: 2.3571505546569824\n",
      "  batch 3 loss: 2.4646689891815186\n",
      "  batch 4 loss: 2.635538101196289\n",
      "  batch 5 loss: 2.6179428100585938\n",
      "  batch 6 loss: 2.8356387615203857\n",
      "  batch 7 loss: 2.584561824798584\n",
      "  batch 8 loss: 2.5715548992156982\n",
      "  batch 9 loss: 2.3742730617523193\n",
      "  batch 10 loss: 2.4960927963256836\n",
      "  batch 11 loss: 2.7197792530059814\n",
      "  batch 12 loss: 2.7744531631469727\n",
      "  batch 13 loss: 2.4131557941436768\n",
      "  batch 14 loss: 2.5599989891052246\n",
      "  batch 15 loss: 2.843081474304199\n",
      "  batch 16 loss: 2.5754873752593994\n",
      "  batch 17 loss: 2.358093023300171\n",
      "  batch 18 loss: 2.6434121131896973\n",
      "  batch 19 loss: 2.5016870498657227\n",
      "  batch 20 loss: 2.363510847091675\n",
      "  batch 21 loss: 2.5192830562591553\n",
      "  batch 22 loss: 2.5925989151000977\n",
      "  batch 23 loss: 2.5994553565979004\n",
      "  batch 24 loss: 2.6678860187530518\n",
      "  batch 25 loss: 2.772209405899048\n",
      "  batch 26 loss: 2.7613680362701416\n",
      "  batch 27 loss: 2.6203954219818115\n",
      "  batch 28 loss: 2.2072691917419434\n",
      "  batch 29 loss: 2.5168309211730957\n",
      "  batch 30 loss: 2.7126591205596924\n",
      "  batch 31 loss: 2.5157573223114014\n",
      "  batch 32 loss: 2.558757781982422\n",
      "  batch 33 loss: 2.7986369132995605\n",
      "  batch 34 loss: 2.3184146881103516\n",
      "  batch 35 loss: 2.4019858837127686\n",
      "  batch 36 loss: 2.329075813293457\n",
      "  batch 37 loss: 2.2691006660461426\n",
      "  batch 38 loss: 2.453307867050171\n",
      "  batch 39 loss: 2.537595748901367\n",
      "  batch 40 loss: 2.384277820587158\n",
      "  batch 41 loss: 2.6072299480438232\n",
      "  batch 42 loss: 2.4307034015655518\n",
      "  batch 43 loss: 2.189586639404297\n",
      "  batch 44 loss: 2.5036158561706543\n",
      "  batch 45 loss: 2.492133617401123\n",
      "  batch 46 loss: 2.6066219806671143\n",
      "  batch 47 loss: 2.51670503616333\n",
      "  batch 48 loss: 2.42242693901062\n",
      "  batch 49 loss: 2.437335729598999\n",
      "  batch 50 loss: 2.404456615447998\n",
      "  batch 51 loss: 3.0188708305358887\n",
      "  batch 52 loss: 2.574436664581299\n",
      "  batch 53 loss: 2.571772813796997\n",
      "  batch 54 loss: 2.211793899536133\n",
      "  batch 55 loss: 2.6858739852905273\n",
      "  batch 56 loss: 2.6336960792541504\n",
      "  batch 57 loss: 2.547471284866333\n",
      "  batch 58 loss: 2.4012598991394043\n",
      "  batch 59 loss: 2.411001443862915\n",
      "  batch 60 loss: 2.6289029121398926\n",
      "  batch 61 loss: 2.5912959575653076\n",
      "  batch 62 loss: 2.5010628700256348\n",
      "  batch 63 loss: 2.475975275039673\n",
      "  batch 64 loss: 2.5439014434814453\n",
      "  batch 65 loss: 2.5632669925689697\n",
      "  batch 66 loss: 2.4934847354888916\n",
      "  batch 67 loss: 2.207847833633423\n",
      "  batch 68 loss: 2.5901529788970947\n",
      "  batch 69 loss: 2.3224635124206543\n",
      "  batch 70 loss: 2.6367921829223633\n",
      "  batch 71 loss: 2.510876178741455\n",
      "  batch 72 loss: 2.6921751499176025\n",
      "  batch 73 loss: 2.194275140762329\n",
      "  batch 74 loss: 2.421921491622925\n",
      "  batch 75 loss: 2.3402163982391357\n",
      "  batch 76 loss: 2.4286491870880127\n",
      "  batch 77 loss: 2.674922466278076\n",
      "  batch 78 loss: 2.5768895149230957\n",
      "  batch 79 loss: 2.484269857406616\n",
      "  batch 80 loss: 2.4937515258789062\n",
      "  batch 81 loss: 2.497370719909668\n",
      "  batch 82 loss: 2.3050453662872314\n",
      "  batch 83 loss: 2.4446847438812256\n",
      "  batch 84 loss: 2.314408540725708\n",
      "  batch 85 loss: 2.291592597961426\n",
      "  batch 86 loss: 2.3550374507904053\n",
      "  batch 87 loss: 2.230660915374756\n",
      "  batch 88 loss: 2.232572078704834\n",
      "  batch 89 loss: 2.6782262325286865\n",
      "  batch 90 loss: 2.719118595123291\n",
      "  batch 91 loss: 2.633298635482788\n",
      "  batch 92 loss: 2.43818998336792\n",
      "  batch 93 loss: 2.472257137298584\n",
      "  batch 94 loss: 2.494715690612793\n",
      "  batch 95 loss: 2.278547525405884\n",
      "  batch 96 loss: 2.3268465995788574\n",
      "  batch 97 loss: 2.507415771484375\n",
      "  batch 98 loss: 2.5475077629089355\n",
      "  batch 99 loss: 2.16902232170105\n",
      "  batch 100 loss: 2.818415880203247\n",
      "  batch 101 loss: 2.3736016750335693\n",
      "  batch 102 loss: 2.3394827842712402\n",
      "  batch 103 loss: 2.5379116535186768\n",
      "  batch 104 loss: 2.287175178527832\n",
      "  batch 105 loss: 2.411266326904297\n",
      "  batch 106 loss: 2.474175453186035\n",
      "  batch 107 loss: 2.49468994140625\n",
      "  batch 108 loss: 2.516573667526245\n",
      "  batch 109 loss: 2.471557140350342\n",
      "  batch 110 loss: 2.42441463470459\n",
      "  batch 111 loss: 2.2475838661193848\n",
      "  batch 112 loss: 2.3254153728485107\n",
      "  batch 113 loss: 2.2972986698150635\n",
      "  batch 114 loss: 2.5005502700805664\n",
      "  batch 115 loss: 2.463953733444214\n",
      "  batch 116 loss: 2.8213396072387695\n",
      "  batch 1 loss: 2.2841897010803223\n",
      "  batch 2 loss: 2.1504836082458496\n",
      "  batch 3 loss: 2.313422441482544\n",
      "  batch 4 loss: 2.5382747650146484\n",
      "  batch 5 loss: 2.5122532844543457\n",
      "  batch 6 loss: 2.7030067443847656\n",
      "  batch 7 loss: 2.52046799659729\n",
      "  batch 8 loss: 2.4330625534057617\n",
      "  batch 9 loss: 2.2681727409362793\n",
      "  batch 10 loss: 2.4146370887756348\n",
      "  batch 11 loss: 2.6248373985290527\n",
      "  batch 12 loss: 2.6309144496917725\n",
      "  batch 13 loss: 2.2741594314575195\n",
      "  batch 14 loss: 2.421095371246338\n",
      "  batch 15 loss: 2.714179277420044\n",
      "  batch 16 loss: 2.4614691734313965\n",
      "  batch 17 loss: 2.199821949005127\n",
      "  batch 18 loss: 2.525050640106201\n",
      "  batch 19 loss: 2.4109609127044678\n",
      "  batch 20 loss: 2.2932746410369873\n",
      "  batch 21 loss: 2.383571147918701\n",
      "  batch 22 loss: 2.5575215816497803\n",
      "  batch 23 loss: 2.581214427947998\n",
      "  batch 24 loss: 2.554232358932495\n",
      "  batch 25 loss: 2.607954978942871\n",
      "  batch 26 loss: 2.673305034637451\n",
      "  batch 27 loss: 2.5068182945251465\n",
      "  batch 28 loss: 2.000601291656494\n",
      "  batch 29 loss: 2.3834359645843506\n",
      "  batch 30 loss: 2.6434316635131836\n",
      "  batch 31 loss: 2.4430806636810303\n",
      "  batch 32 loss: 2.5085887908935547\n",
      "  batch 33 loss: 2.686110019683838\n",
      "  batch 34 loss: 2.1824302673339844\n",
      "  batch 35 loss: 2.319077491760254\n",
      "  batch 36 loss: 2.2100257873535156\n",
      "  batch 37 loss: 2.1339709758758545\n",
      "  batch 38 loss: 2.3814609050750732\n",
      "  batch 39 loss: 2.4492714405059814\n",
      "  batch 40 loss: 2.277221202850342\n",
      "  batch 41 loss: 2.5892553329467773\n",
      "  batch 42 loss: 2.4313712120056152\n",
      "  batch 43 loss: 2.0473668575286865\n",
      "  batch 44 loss: 2.4192323684692383\n",
      "  batch 45 loss: 2.3884494304656982\n",
      "  batch 46 loss: 2.543278932571411\n",
      "  batch 47 loss: 2.473607063293457\n",
      "  batch 48 loss: 2.3324148654937744\n",
      "  batch 49 loss: 2.3901939392089844\n",
      "  batch 50 loss: 2.252641439437866\n",
      "  batch 51 loss: 2.9564414024353027\n",
      "  batch 52 loss: 2.430636405944824\n",
      "  batch 53 loss: 2.4536893367767334\n",
      "  batch 54 loss: 2.0925254821777344\n",
      "  batch 55 loss: 2.56406831741333\n",
      "  batch 56 loss: 2.4465396404266357\n",
      "  batch 57 loss: 2.4335150718688965\n",
      "  batch 58 loss: 2.303633213043213\n",
      "  batch 59 loss: 2.2607975006103516\n",
      "  batch 60 loss: 2.5137360095977783\n",
      "  batch 61 loss: 2.51853609085083\n",
      "  batch 62 loss: 2.4537899494171143\n",
      "  batch 63 loss: 2.390880584716797\n",
      "  batch 64 loss: 2.428776502609253\n",
      "  batch 65 loss: 2.396796941757202\n",
      "  batch 66 loss: 2.460148572921753\n",
      "  batch 67 loss: 2.0997300148010254\n",
      "  batch 68 loss: 2.5116777420043945\n",
      "  batch 69 loss: 2.188699245452881\n",
      "  batch 70 loss: 2.6739349365234375\n",
      "  batch 71 loss: 2.365635633468628\n",
      "  batch 72 loss: 2.7263224124908447\n",
      "  batch 73 loss: 2.1409754753112793\n",
      "  batch 74 loss: 2.410281181335449\n",
      "  batch 75 loss: 2.2905168533325195\n",
      "  batch 76 loss: 2.395195245742798\n",
      "  batch 77 loss: 2.6133415699005127\n",
      "  batch 78 loss: 2.4615488052368164\n",
      "  batch 79 loss: 2.41385817527771\n",
      "  batch 80 loss: 2.448143243789673\n",
      "  batch 81 loss: 2.458887815475464\n",
      "  batch 82 loss: 2.2379519939422607\n",
      "  batch 83 loss: 2.4217939376831055\n",
      "  batch 84 loss: 2.2142791748046875\n",
      "  batch 85 loss: 2.228259801864624\n",
      "  batch 86 loss: 2.30495548248291\n",
      "  batch 87 loss: 2.1854310035705566\n",
      "  batch 88 loss: 2.1499109268188477\n",
      "  batch 89 loss: 2.6018946170806885\n",
      "  batch 90 loss: 2.6315338611602783\n",
      "  batch 91 loss: 2.571932077407837\n",
      "  batch 92 loss: 2.3921031951904297\n",
      "  batch 93 loss: 2.4421207904815674\n",
      "  batch 94 loss: 2.470749855041504\n",
      "  batch 95 loss: 2.2532718181610107\n",
      "  batch 96 loss: 2.24041485786438\n",
      "  batch 97 loss: 2.445964813232422\n",
      "  batch 98 loss: 2.5424394607543945\n",
      "  batch 99 loss: 2.0764143466949463\n",
      "  batch 100 loss: 2.7748894691467285\n",
      "  batch 101 loss: 2.339625120162964\n",
      "  batch 102 loss: 2.2388529777526855\n",
      "  batch 103 loss: 2.485773801803589\n",
      "  batch 104 loss: 2.2325525283813477\n",
      "  batch 105 loss: 2.3333542346954346\n",
      "  batch 106 loss: 2.438060760498047\n",
      "  batch 107 loss: 2.444023609161377\n",
      "  batch 108 loss: 2.504751205444336\n",
      "  batch 109 loss: 2.4831302165985107\n",
      "  batch 110 loss: 2.4227771759033203\n",
      "  batch 111 loss: 2.2234206199645996\n",
      "  batch 112 loss: 2.2703943252563477\n",
      "  batch 113 loss: 2.2774295806884766\n",
      "  batch 114 loss: 2.4425065517425537\n",
      "  batch 115 loss: 2.4275081157684326\n",
      "  batch 116 loss: 2.8543338775634766\n",
      "  batch 1 loss: 2.241891860961914\n",
      "  batch 2 loss: 2.110445261001587\n",
      "  batch 3 loss: 2.3038034439086914\n",
      "  batch 4 loss: 2.4946305751800537\n",
      "  batch 5 loss: 2.4922597408294678\n",
      "  batch 6 loss: 2.672170639038086\n",
      "  batch 7 loss: 2.4629271030426025\n",
      "  batch 8 loss: 2.4116969108581543\n",
      "  batch 9 loss: 2.2297661304473877\n",
      "  batch 10 loss: 2.3663170337677\n",
      "  batch 11 loss: 2.5794925689697266\n",
      "  batch 12 loss: 2.550466299057007\n",
      "  batch 13 loss: 2.253495931625366\n",
      "  batch 14 loss: 2.4031026363372803\n",
      "  batch 15 loss: 2.67507266998291\n",
      "  batch 16 loss: 2.4135336875915527\n",
      "  batch 17 loss: 2.1483943462371826\n",
      "  batch 18 loss: 2.449416399002075\n",
      "  batch 19 loss: 2.3965792655944824\n",
      "  batch 20 loss: 2.2737350463867188\n",
      "  batch 21 loss: 2.367469310760498\n",
      "  batch 22 loss: 2.553297996520996\n",
      "  batch 23 loss: 2.571547031402588\n",
      "  batch 24 loss: 2.534539222717285\n",
      "  batch 25 loss: 2.531306505203247\n",
      "  batch 26 loss: 2.6444485187530518\n",
      "  batch 27 loss: 2.4689316749572754\n",
      "  batch 28 loss: 1.9895803928375244\n",
      "  batch 29 loss: 2.3446199893951416\n",
      "  batch 30 loss: 2.5923686027526855\n",
      "  batch 31 loss: 2.3878517150878906\n",
      "  batch 32 loss: 2.4613027572631836\n",
      "  batch 33 loss: 2.632704496383667\n",
      "  batch 34 loss: 2.1649160385131836\n",
      "  batch 35 loss: 2.3117682933807373\n",
      "  batch 36 loss: 2.2056210041046143\n",
      "  batch 37 loss: 2.128020763397217\n",
      "  batch 38 loss: 2.365481376647949\n",
      "  batch 39 loss: 2.4472970962524414\n",
      "  batch 40 loss: 2.2055599689483643\n",
      "  batch 41 loss: 2.549665927886963\n",
      "  batch 42 loss: 2.435939311981201\n",
      "  batch 43 loss: 2.0284366607666016\n",
      "  batch 44 loss: 2.381427764892578\n",
      "  batch 45 loss: 2.3434712886810303\n",
      "  batch 46 loss: 2.5116679668426514\n",
      "  batch 47 loss: 2.437624931335449\n",
      "  batch 48 loss: 2.294577121734619\n",
      "  batch 49 loss: 2.329620122909546\n",
      "  batch 50 loss: 2.230424642562866\n",
      "  batch 51 loss: 2.9329867362976074\n",
      "  batch 52 loss: 2.3963654041290283\n",
      "  batch 53 loss: 2.410818099975586\n",
      "  batch 54 loss: 2.0557668209075928\n",
      "  batch 55 loss: 2.513458013534546\n",
      "  batch 56 loss: 2.4154369831085205\n",
      "  batch 57 loss: 2.388735294342041\n",
      "  batch 58 loss: 2.270479440689087\n",
      "  batch 59 loss: 2.230316638946533\n",
      "  batch 60 loss: 2.487619161605835\n",
      "  batch 61 loss: 2.483579158782959\n",
      "  batch 62 loss: 2.427152633666992\n",
      "  batch 63 loss: 2.3750534057617188\n",
      "  batch 64 loss: 2.4222571849823\n",
      "  batch 65 loss: 2.358015775680542\n",
      "  batch 66 loss: 2.441221237182617\n",
      "  batch 67 loss: 2.0678329467773438\n",
      "  batch 68 loss: 2.4676003456115723\n",
      "  batch 69 loss: 2.151358127593994\n",
      "  batch 70 loss: 2.6622374057769775\n",
      "  batch 71 loss: 2.320695400238037\n",
      "  batch 72 loss: 2.689361572265625\n",
      "  batch 73 loss: 2.1236939430236816\n",
      "  batch 74 loss: 2.393562078475952\n",
      "  batch 75 loss: 2.2663605213165283\n",
      "  batch 76 loss: 2.3594934940338135\n",
      "  batch 77 loss: 2.5946621894836426\n",
      "  batch 78 loss: 2.4274425506591797\n",
      "  batch 79 loss: 2.402913808822632\n",
      "  batch 80 loss: 2.437387466430664\n",
      "  batch 81 loss: 2.4226789474487305\n",
      "  batch 82 loss: 2.204657793045044\n",
      "  batch 83 loss: 2.418743371963501\n",
      "  batch 84 loss: 2.1757566928863525\n",
      "  batch 85 loss: 2.191112756729126\n",
      "  batch 86 loss: 2.2915830612182617\n",
      "  batch 87 loss: 2.1654062271118164\n",
      "  batch 88 loss: 2.1443862915039062\n",
      "  batch 89 loss: 2.5773918628692627\n",
      "  batch 90 loss: 2.5888779163360596\n",
      "  batch 91 loss: 2.5551249980926514\n",
      "  batch 92 loss: 2.360006093978882\n",
      "  batch 93 loss: 2.4253621101379395\n",
      "  batch 94 loss: 2.4325339794158936\n",
      "  batch 95 loss: 2.229686737060547\n",
      "  batch 96 loss: 2.219475030899048\n",
      "  batch 97 loss: 2.416037082672119\n",
      "  batch 98 loss: 2.532892942428589\n",
      "  batch 99 loss: 2.042273998260498\n",
      "  batch 100 loss: 2.7395942211151123\n",
      "  batch 101 loss: 2.3147687911987305\n",
      "  batch 102 loss: 2.2039952278137207\n",
      "  batch 103 loss: 2.46516489982605\n",
      "  batch 104 loss: 2.2114713191986084\n",
      "  batch 105 loss: 2.3015031814575195\n",
      "  batch 106 loss: 2.4151196479797363\n",
      "  batch 107 loss: 2.4309425354003906\n",
      "  batch 108 loss: 2.4748380184173584\n",
      "  batch 109 loss: 2.4681320190429688\n",
      "  batch 110 loss: 2.3903865814208984\n",
      "  batch 111 loss: 2.2074291706085205\n",
      "  batch 112 loss: 2.228243589401245\n",
      "  batch 113 loss: 2.2595224380493164\n",
      "  batch 114 loss: 2.427616596221924\n",
      "  batch 115 loss: 2.39731502532959\n",
      "  batch 116 loss: 2.853707790374756\n",
      "  batch 1 loss: 2.1992125511169434\n",
      "  batch 2 loss: 2.084678888320923\n",
      "  batch 3 loss: 2.2676494121551514\n",
      "  batch 4 loss: 2.504187822341919\n",
      "  batch 5 loss: 2.470700979232788\n",
      "  batch 6 loss: 2.6499485969543457\n",
      "  batch 7 loss: 2.427628993988037\n",
      "  batch 8 loss: 2.380849838256836\n",
      "  batch 9 loss: 2.2137162685394287\n",
      "  batch 10 loss: 2.354391574859619\n",
      "  batch 11 loss: 2.558558940887451\n",
      "  batch 12 loss: 2.537155866622925\n",
      "  batch 13 loss: 2.2447190284729004\n",
      "  batch 14 loss: 2.4007859230041504\n",
      "  batch 15 loss: 2.6306984424591064\n",
      "  batch 16 loss: 2.379696846008301\n",
      "  batch 17 loss: 2.1426961421966553\n",
      "  batch 18 loss: 2.42476749420166\n",
      "  batch 19 loss: 2.378335475921631\n",
      "  batch 20 loss: 2.272709369659424\n",
      "  batch 21 loss: 2.3717334270477295\n",
      "  batch 22 loss: 2.5379111766815186\n",
      "  batch 23 loss: 2.5615625381469727\n",
      "  batch 24 loss: 2.5111758708953857\n",
      "  batch 25 loss: 2.506209373474121\n",
      "  batch 26 loss: 2.6307125091552734\n",
      "  batch 27 loss: 2.4364147186279297\n",
      "  batch 28 loss: 1.981618881225586\n",
      "  batch 29 loss: 2.2991631031036377\n",
      "  batch 30 loss: 2.541097640991211\n",
      "  batch 31 loss: 2.3564653396606445\n",
      "  batch 32 loss: 2.433483123779297\n",
      "  batch 33 loss: 2.594027280807495\n",
      "  batch 34 loss: 2.144894599914551\n",
      "  batch 35 loss: 2.303069829940796\n",
      "  batch 36 loss: 2.1845486164093018\n",
      "  batch 37 loss: 2.117370843887329\n",
      "  batch 38 loss: 2.3556697368621826\n",
      "  batch 39 loss: 2.4167864322662354\n",
      "  batch 40 loss: 2.1939055919647217\n",
      "  batch 41 loss: 2.5090456008911133\n",
      "  batch 42 loss: 2.4379804134368896\n",
      "  batch 43 loss: 2.000281810760498\n",
      "  batch 44 loss: 2.3688652515411377\n",
      "  batch 45 loss: 2.3383805751800537\n",
      "  batch 46 loss: 2.508493661880493\n",
      "  batch 47 loss: 2.402848482131958\n",
      "  batch 48 loss: 2.271869421005249\n",
      "  batch 49 loss: 2.2989954948425293\n",
      "  batch 50 loss: 2.2207424640655518\n",
      "  batch 51 loss: 2.900599479675293\n",
      "  batch 52 loss: 2.3679771423339844\n",
      "  batch 53 loss: 2.3746087551116943\n",
      "  batch 54 loss: 2.0342445373535156\n",
      "  batch 55 loss: 2.448319435119629\n",
      "  batch 56 loss: 2.3922483921051025\n",
      "  batch 57 loss: 2.412170648574829\n",
      "  batch 58 loss: 2.261533260345459\n",
      "  batch 59 loss: 2.1890900135040283\n",
      "  batch 60 loss: 2.4520034790039062\n",
      "  batch 61 loss: 2.4404923915863037\n",
      "  batch 62 loss: 2.409998655319214\n",
      "  batch 63 loss: 2.3731796741485596\n",
      "  batch 64 loss: 2.452773332595825\n",
      "  batch 65 loss: 2.3526294231414795\n",
      "  batch 66 loss: 2.388704538345337\n",
      "  batch 67 loss: 2.0719351768493652\n",
      "  batch 68 loss: 2.444765090942383\n",
      "  batch 69 loss: 2.157811164855957\n",
      "  batch 70 loss: 2.6228272914886475\n",
      "  batch 71 loss: 2.2804224491119385\n",
      "  batch 72 loss: 2.625316858291626\n",
      "  batch 73 loss: 2.1274898052215576\n",
      "  batch 74 loss: 2.357989549636841\n",
      "  batch 75 loss: 2.247478723526001\n",
      "  batch 76 loss: 2.3394508361816406\n",
      "  batch 77 loss: 2.576328754425049\n",
      "  batch 78 loss: 2.3901584148406982\n",
      "  batch 79 loss: 2.3519327640533447\n",
      "  batch 80 loss: 2.405691146850586\n",
      "  batch 81 loss: 2.3799538612365723\n",
      "  batch 82 loss: 2.196753978729248\n",
      "  batch 83 loss: 2.4041380882263184\n",
      "  batch 84 loss: 2.1582891941070557\n",
      "  batch 85 loss: 2.1422860622406006\n",
      "  batch 86 loss: 2.2908198833465576\n",
      "  batch 87 loss: 2.147183895111084\n",
      "  batch 88 loss: 2.145787477493286\n",
      "  batch 89 loss: 2.531965732574463\n",
      "  batch 90 loss: 2.528369188308716\n",
      "  batch 91 loss: 2.5233805179595947\n",
      "  batch 92 loss: 2.304990768432617\n",
      "  batch 93 loss: 2.3908562660217285\n",
      "  batch 94 loss: 2.3770174980163574\n",
      "  batch 95 loss: 2.2288646697998047\n",
      "  batch 96 loss: 2.170450448989868\n",
      "  batch 97 loss: 2.3954918384552\n",
      "  batch 98 loss: 2.5383567810058594\n",
      "  batch 99 loss: 2.015597105026245\n",
      "  batch 100 loss: 2.648453950881958\n",
      "  batch 101 loss: 2.306333303451538\n",
      "  batch 102 loss: 2.154240369796753\n",
      "  batch 103 loss: 2.3937864303588867\n",
      "  batch 104 loss: 2.2072534561157227\n",
      "  batch 105 loss: 2.2696049213409424\n",
      "  batch 106 loss: 2.3683855533599854\n",
      "  batch 107 loss: 2.39697265625\n",
      "  batch 108 loss: 2.3870272636413574\n",
      "  batch 109 loss: 2.40346097946167\n",
      "  batch 110 loss: 2.3028926849365234\n",
      "  batch 111 loss: 2.1884605884552\n",
      "  batch 112 loss: 2.1840860843658447\n",
      "  batch 113 loss: 2.212656021118164\n",
      "  batch 114 loss: 2.3990683555603027\n",
      "  batch 115 loss: 2.3271636962890625\n",
      "  batch 116 loss: 2.7847166061401367\n",
      "  batch 1 loss: 2.22951078414917\n",
      "  batch 2 loss: 2.1277804374694824\n",
      "  batch 3 loss: 2.217996597290039\n",
      "  batch 4 loss: 2.4435267448425293\n",
      "  batch 5 loss: 2.3515868186950684\n",
      "  batch 6 loss: 2.5765504837036133\n",
      "  batch 7 loss: 2.3204383850097656\n",
      "  batch 8 loss: 2.3211727142333984\n",
      "  batch 9 loss: 2.1408960819244385\n",
      "  batch 10 loss: 2.283184051513672\n",
      "  batch 11 loss: 2.475269079208374\n",
      "  batch 12 loss: 2.4855711460113525\n",
      "  batch 13 loss: 2.227071523666382\n",
      "  batch 14 loss: 2.303262710571289\n",
      "  batch 15 loss: 2.5385119915008545\n",
      "  batch 16 loss: 2.3350422382354736\n",
      "  batch 17 loss: 2.108469009399414\n",
      "  batch 18 loss: 2.3903684616088867\n",
      "  batch 19 loss: 2.3079047203063965\n",
      "  batch 20 loss: 2.22564959526062\n",
      "  batch 21 loss: 2.308537721633911\n",
      "  batch 22 loss: 2.459739923477173\n",
      "  batch 23 loss: 2.532569169998169\n",
      "  batch 24 loss: 2.425532817840576\n",
      "  batch 25 loss: 2.427790880203247\n",
      "  batch 26 loss: 2.5402069091796875\n",
      "  batch 27 loss: 2.3525121212005615\n",
      "  batch 28 loss: 2.0123069286346436\n",
      "  batch 29 loss: 2.217647075653076\n",
      "  batch 30 loss: 2.492149591445923\n",
      "  batch 31 loss: 2.349842071533203\n",
      "  batch 32 loss: 2.4298605918884277\n",
      "  batch 33 loss: 2.6475255489349365\n",
      "  batch 34 loss: 2.0715432167053223\n",
      "  batch 35 loss: 2.3075945377349854\n",
      "  batch 36 loss: 2.120208501815796\n",
      "  batch 37 loss: 2.075890064239502\n",
      "  batch 38 loss: 2.3340396881103516\n",
      "  batch 39 loss: 2.36794114112854\n",
      "  batch 40 loss: 2.1624491214752197\n",
      "  batch 41 loss: 2.43487811088562\n",
      "  batch 42 loss: 2.4155921936035156\n",
      "  batch 43 loss: 1.9617114067077637\n",
      "  batch 44 loss: 2.2803192138671875\n",
      "  batch 45 loss: 2.284095048904419\n",
      "  batch 46 loss: 2.477195978164673\n",
      "  batch 47 loss: 2.3143179416656494\n",
      "  batch 48 loss: 2.2713332176208496\n",
      "  batch 49 loss: 2.2916839122772217\n",
      "  batch 50 loss: 2.2144181728363037\n",
      "  batch 51 loss: 2.738637685775757\n",
      "  batch 52 loss: 2.293997287750244\n",
      "  batch 53 loss: 2.3313705921173096\n",
      "  batch 54 loss: 2.013441562652588\n",
      "  batch 55 loss: 2.403088092803955\n",
      "  batch 56 loss: 2.322916269302368\n",
      "  batch 57 loss: 2.294405698776245\n",
      "  batch 58 loss: 2.2056634426116943\n",
      "  batch 59 loss: 2.139617443084717\n",
      "  batch 60 loss: 2.456041097640991\n",
      "  batch 61 loss: 2.4329962730407715\n",
      "  batch 62 loss: 2.365443706512451\n",
      "  batch 63 loss: 2.3174893856048584\n",
      "  batch 64 loss: 2.3499672412872314\n",
      "  batch 65 loss: 2.2711801528930664\n",
      "  batch 66 loss: 2.3404335975646973\n",
      "  batch 67 loss: 1.9752079248428345\n",
      "  batch 68 loss: 2.349297046661377\n",
      "  batch 69 loss: 2.112530469894409\n",
      "  batch 70 loss: 2.4929237365722656\n",
      "  batch 71 loss: 2.191160202026367\n",
      "  batch 72 loss: 2.4599525928497314\n",
      "  batch 73 loss: 2.0397186279296875\n",
      "  batch 74 loss: 2.269778251647949\n",
      "  batch 75 loss: 2.144840955734253\n",
      "  batch 76 loss: 2.2537806034088135\n",
      "  batch 77 loss: 2.534097194671631\n",
      "  batch 78 loss: 2.3084280490875244\n",
      "  batch 79 loss: 2.275373935699463\n",
      "  batch 80 loss: 2.3712925910949707\n",
      "  batch 81 loss: 2.34537935256958\n",
      "  batch 82 loss: 2.1819908618927\n",
      "  batch 83 loss: 2.347106456756592\n",
      "  batch 84 loss: 2.1019704341888428\n",
      "  batch 85 loss: 2.0462207794189453\n",
      "  batch 86 loss: 2.2939443588256836\n",
      "  batch 87 loss: 2.1375107765197754\n",
      "  batch 88 loss: 2.0471956729888916\n",
      "  batch 89 loss: 2.4265060424804688\n",
      "  batch 90 loss: 2.433255910873413\n",
      "  batch 91 loss: 2.466578245162964\n",
      "  batch 92 loss: 2.2213642597198486\n",
      "  batch 93 loss: 2.346374750137329\n",
      "  batch 94 loss: 2.318476915359497\n",
      "  batch 95 loss: 2.098137855529785\n",
      "  batch 96 loss: 2.0957326889038086\n",
      "  batch 97 loss: 2.2985522747039795\n",
      "  batch 98 loss: 2.457549810409546\n",
      "  batch 99 loss: 1.9463021755218506\n",
      "  batch 100 loss: 2.5650908946990967\n",
      "  batch 101 loss: 2.2763099670410156\n",
      "  batch 102 loss: 2.0891780853271484\n",
      "  batch 103 loss: 2.3266334533691406\n",
      "  batch 104 loss: 2.1525213718414307\n",
      "  batch 105 loss: 2.1547603607177734\n",
      "  batch 106 loss: 2.309204578399658\n",
      "  batch 107 loss: 2.3517613410949707\n",
      "  batch 108 loss: 2.3177053928375244\n",
      "  batch 109 loss: 2.3572511672973633\n",
      "  batch 110 loss: 2.2669899463653564\n",
      "  batch 111 loss: 2.1604700088500977\n",
      "  batch 112 loss: 2.1300179958343506\n",
      "  batch 113 loss: 2.1388535499572754\n",
      "  batch 114 loss: 2.3291258811950684\n",
      "  batch 115 loss: 2.2912113666534424\n",
      "  batch 116 loss: 2.8192193508148193\n",
      "  batch 1 loss: 2.3554065227508545\n",
      "  batch 2 loss: 2.325620651245117\n",
      "  batch 3 loss: 2.2368483543395996\n",
      "  batch 4 loss: 2.347108840942383\n",
      "  batch 5 loss: 2.2827346324920654\n",
      "  batch 6 loss: 2.564065933227539\n",
      "  batch 7 loss: 2.3390374183654785\n",
      "  batch 8 loss: 2.3084707260131836\n",
      "  batch 9 loss: 2.1088457107543945\n",
      "  batch 10 loss: 2.2482118606567383\n",
      "  batch 11 loss: 2.424733877182007\n",
      "  batch 12 loss: 2.423434257507324\n",
      "  batch 13 loss: 2.163590669631958\n",
      "  batch 14 loss: 2.2723522186279297\n",
      "  batch 15 loss: 2.4773359298706055\n",
      "  batch 16 loss: 2.2925918102264404\n",
      "  batch 17 loss: 2.0969042778015137\n",
      "  batch 18 loss: 2.353394031524658\n",
      "  batch 19 loss: 2.225294828414917\n",
      "  batch 20 loss: 2.195220947265625\n",
      "  batch 21 loss: 2.277034282684326\n",
      "  batch 22 loss: 2.4011001586914062\n",
      "  batch 23 loss: 2.4483652114868164\n",
      "  batch 24 loss: 2.3509645462036133\n",
      "  batch 25 loss: 2.338521718978882\n",
      "  batch 26 loss: 2.542271852493286\n",
      "  batch 27 loss: 2.3439977169036865\n",
      "  batch 28 loss: 2.0075831413269043\n",
      "  batch 29 loss: 2.157297372817993\n",
      "  batch 30 loss: 2.458191394805908\n",
      "  batch 31 loss: 2.293849468231201\n",
      "  batch 32 loss: 2.4239776134490967\n",
      "  batch 33 loss: 2.601548910140991\n",
      "  batch 34 loss: 2.06948184967041\n",
      "  batch 35 loss: 2.236024856567383\n",
      "  batch 36 loss: 2.0551810264587402\n",
      "  batch 37 loss: 2.08073353767395\n",
      "  batch 38 loss: 2.333322286605835\n",
      "  batch 39 loss: 2.294670581817627\n",
      "  batch 40 loss: 2.086918354034424\n",
      "  batch 41 loss: 2.3929712772369385\n",
      "  batch 42 loss: 2.3812568187713623\n",
      "  batch 43 loss: 1.9121782779693604\n",
      "  batch 44 loss: 2.225639820098877\n",
      "  batch 45 loss: 2.258334159851074\n",
      "  batch 46 loss: 2.4321072101593018\n",
      "  batch 47 loss: 2.2673425674438477\n",
      "  batch 48 loss: 2.231976270675659\n",
      "  batch 49 loss: 2.2419440746307373\n",
      "  batch 50 loss: 2.200232982635498\n",
      "  batch 51 loss: 2.568450689315796\n",
      "  batch 52 loss: 2.2598495483398438\n",
      "  batch 53 loss: 2.308934450149536\n",
      "  batch 54 loss: 1.9805004596710205\n",
      "  batch 55 loss: 2.4156734943389893\n",
      "  batch 56 loss: 2.3022022247314453\n",
      "  batch 57 loss: 2.218095064163208\n",
      "  batch 58 loss: 2.1724014282226562\n",
      "  batch 59 loss: 2.1056630611419678\n",
      "  batch 60 loss: 2.4414796829223633\n",
      "  batch 61 loss: 2.409966230392456\n",
      "  batch 62 loss: 2.3356759548187256\n",
      "  batch 63 loss: 2.279144763946533\n",
      "  batch 64 loss: 2.1829922199249268\n",
      "  batch 65 loss: 2.1490790843963623\n",
      "  batch 66 loss: 2.2789554595947266\n",
      "  batch 67 loss: 1.9279732704162598\n",
      "  batch 68 loss: 2.328420877456665\n",
      "  batch 69 loss: 2.0782618522644043\n",
      "  batch 70 loss: 2.4296181201934814\n",
      "  batch 71 loss: 2.1340115070343018\n",
      "  batch 72 loss: 2.388474225997925\n",
      "  batch 73 loss: 2.005323886871338\n",
      "  batch 74 loss: 2.164175271987915\n",
      "  batch 75 loss: 2.070373296737671\n",
      "  batch 76 loss: 2.232429265975952\n",
      "  batch 77 loss: 2.488673686981201\n",
      "  batch 78 loss: 2.303889036178589\n",
      "  batch 79 loss: 2.321812629699707\n",
      "  batch 80 loss: 2.351361036300659\n",
      "  batch 81 loss: 2.3125767707824707\n",
      "  batch 82 loss: 2.148433208465576\n",
      "  batch 83 loss: 2.357907295227051\n",
      "  batch 84 loss: 2.0768468379974365\n",
      "  batch 85 loss: 2.0304064750671387\n",
      "  batch 86 loss: 2.3013906478881836\n",
      "  batch 87 loss: 2.079465627670288\n",
      "  batch 88 loss: 2.0774502754211426\n",
      "  batch 89 loss: 2.4385037422180176\n",
      "  batch 90 loss: 2.445579767227173\n",
      "  batch 91 loss: 2.3885438442230225\n",
      "  batch 92 loss: 2.1690313816070557\n",
      "  batch 93 loss: 2.3595218658447266\n",
      "  batch 94 loss: 2.278827667236328\n",
      "  batch 95 loss: 2.108799457550049\n",
      "  batch 96 loss: 2.0517661571502686\n",
      "  batch 97 loss: 2.350745439529419\n",
      "  batch 98 loss: 2.4327802658081055\n",
      "  batch 99 loss: 1.9516187906265259\n",
      "  batch 100 loss: 2.557202100753784\n",
      "  batch 101 loss: 2.2549946308135986\n",
      "  batch 102 loss: 2.11163330078125\n",
      "  batch 103 loss: 2.290217638015747\n",
      "  batch 104 loss: 2.125925064086914\n",
      "  batch 105 loss: 2.0939126014709473\n",
      "  batch 106 loss: 2.3204691410064697\n",
      "  batch 107 loss: 2.2881734371185303\n",
      "  batch 108 loss: 2.285440444946289\n",
      "  batch 109 loss: 2.308401346206665\n",
      "  batch 110 loss: 2.2661094665527344\n",
      "  batch 111 loss: 2.1749050617218018\n",
      "  batch 112 loss: 2.0744385719299316\n",
      "  batch 113 loss: 2.1108903884887695\n",
      "  batch 114 loss: 2.301971435546875\n",
      "  batch 115 loss: 2.2619669437408447\n",
      "  batch 116 loss: 2.855241298675537\n",
      "  batch 1 loss: 2.2042908668518066\n",
      "  batch 2 loss: 2.03125\n",
      "  batch 3 loss: 2.182913064956665\n",
      "  batch 4 loss: 2.31158709526062\n",
      "  batch 5 loss: 2.252821207046509\n",
      "  batch 6 loss: 2.619097948074341\n",
      "  batch 7 loss: 2.280618190765381\n",
      "  batch 8 loss: 2.2674200534820557\n",
      "  batch 9 loss: 2.0409390926361084\n",
      "  batch 10 loss: 2.2058184146881104\n",
      "  batch 11 loss: 2.4100537300109863\n",
      "  batch 12 loss: 2.4025065898895264\n",
      "  batch 13 loss: 2.114656925201416\n",
      "  batch 14 loss: 2.2582709789276123\n",
      "  batch 15 loss: 2.4886538982391357\n",
      "  batch 16 loss: 2.1984148025512695\n",
      "  batch 17 loss: 2.0871026515960693\n",
      "  batch 18 loss: 2.281832218170166\n",
      "  batch 19 loss: 2.107053279876709\n",
      "  batch 20 loss: 2.1879262924194336\n",
      "  batch 21 loss: 2.170722007751465\n",
      "  batch 22 loss: 2.3417763710021973\n",
      "  batch 23 loss: 2.417886972427368\n",
      "  batch 24 loss: 2.3208372592926025\n",
      "  batch 25 loss: 2.394580364227295\n",
      "  batch 26 loss: 2.486837148666382\n",
      "  batch 27 loss: 2.2186672687530518\n",
      "  batch 28 loss: 1.935279130935669\n",
      "  batch 29 loss: 2.1187093257904053\n",
      "  batch 30 loss: 2.432506799697876\n",
      "  batch 31 loss: 2.248467206954956\n",
      "  batch 32 loss: 2.3655147552490234\n",
      "  batch 33 loss: 2.4323291778564453\n",
      "  batch 34 loss: 2.063692331314087\n",
      "  batch 35 loss: 2.1979053020477295\n",
      "  batch 36 loss: 1.9943914413452148\n",
      "  batch 37 loss: 2.0089950561523438\n",
      "  batch 38 loss: 2.2586476802825928\n",
      "  batch 39 loss: 2.2459681034088135\n",
      "  batch 40 loss: 2.054755449295044\n",
      "  batch 41 loss: 2.3229997158050537\n",
      "  batch 42 loss: 2.3320069313049316\n",
      "  batch 43 loss: 1.8789024353027344\n",
      "  batch 44 loss: 2.2361626625061035\n",
      "  batch 45 loss: 2.2356762886047363\n",
      "  batch 46 loss: 2.3642666339874268\n",
      "  batch 47 loss: 2.223025321960449\n",
      "  batch 48 loss: 2.2054977416992188\n",
      "  batch 49 loss: 2.214785099029541\n",
      "  batch 50 loss: 2.1646549701690674\n",
      "  batch 51 loss: 2.5005996227264404\n",
      "  batch 52 loss: 2.1819663047790527\n",
      "  batch 53 loss: 2.280843734741211\n",
      "  batch 54 loss: 1.9510164260864258\n",
      "  batch 55 loss: 2.425543785095215\n",
      "  batch 56 loss: 2.2817680835723877\n",
      "  batch 57 loss: 2.1655657291412354\n",
      "  batch 58 loss: 2.1611640453338623\n",
      "  batch 59 loss: 2.05462908744812\n",
      "  batch 60 loss: 2.416623592376709\n",
      "  batch 61 loss: 2.3820953369140625\n",
      "  batch 62 loss: 2.3001861572265625\n",
      "  batch 63 loss: 2.2385470867156982\n",
      "  batch 64 loss: 2.128476858139038\n",
      "  batch 65 loss: 2.109886884689331\n",
      "  batch 66 loss: 2.2222506999969482\n",
      "  batch 67 loss: 1.9100701808929443\n",
      "  batch 68 loss: 2.3049416542053223\n",
      "  batch 69 loss: 2.0448594093322754\n",
      "  batch 70 loss: 2.403768301010132\n",
      "  batch 71 loss: 2.107084035873413\n",
      "  batch 72 loss: 2.3605856895446777\n",
      "  batch 73 loss: 1.9681718349456787\n",
      "  batch 74 loss: 2.1386942863464355\n",
      "  batch 75 loss: 2.0527567863464355\n",
      "  batch 76 loss: 2.208757162094116\n",
      "  batch 77 loss: 2.462350368499756\n",
      "  batch 78 loss: 2.277318000793457\n",
      "  batch 79 loss: 2.2959866523742676\n",
      "  batch 80 loss: 2.3050146102905273\n",
      "  batch 81 loss: 2.268749952316284\n",
      "  batch 82 loss: 2.1023013591766357\n",
      "  batch 83 loss: 2.3046300411224365\n",
      "  batch 84 loss: 2.0614330768585205\n",
      "  batch 85 loss: 2.0205130577087402\n",
      "  batch 86 loss: 2.2772748470306396\n",
      "  batch 87 loss: 2.042292594909668\n",
      "  batch 88 loss: 2.0832602977752686\n",
      "  batch 89 loss: 2.4101102352142334\n",
      "  batch 90 loss: 2.4235916137695312\n",
      "  batch 91 loss: 2.358765125274658\n",
      "  batch 92 loss: 2.133917808532715\n",
      "  batch 93 loss: 2.302884817123413\n",
      "  batch 94 loss: 2.245537519454956\n",
      "  batch 95 loss: 2.0951120853424072\n",
      "  batch 96 loss: 2.0269813537597656\n",
      "  batch 97 loss: 2.3115477561950684\n",
      "  batch 98 loss: 2.3860068321228027\n",
      "  batch 99 loss: 1.9444156885147095\n",
      "  batch 100 loss: 2.5280635356903076\n",
      "  batch 101 loss: 2.2580056190490723\n",
      "  batch 102 loss: 2.0556695461273193\n",
      "  batch 103 loss: 2.22491455078125\n",
      "  batch 104 loss: 2.11240291595459\n",
      "  batch 105 loss: 2.0502545833587646\n",
      "  batch 106 loss: 2.3465523719787598\n",
      "  batch 107 loss: 2.2637627124786377\n",
      "  batch 108 loss: 2.260281562805176\n",
      "  batch 109 loss: 2.2702245712280273\n",
      "  batch 110 loss: 2.2732038497924805\n",
      "  batch 111 loss: 2.1968576908111572\n",
      "  batch 112 loss: 2.073915958404541\n",
      "  batch 113 loss: 2.093722105026245\n",
      "  batch 114 loss: 2.267869472503662\n",
      "  batch 115 loss: 2.237380266189575\n",
      "  batch 116 loss: 2.8441333770751953\n",
      "  batch 1 loss: 2.130833625793457\n",
      "  batch 2 loss: 2.0061194896698\n",
      "  batch 3 loss: 2.1744930744171143\n",
      "  batch 4 loss: 2.2457332611083984\n",
      "  batch 5 loss: 2.1891543865203857\n",
      "  batch 6 loss: 2.5255963802337646\n",
      "  batch 7 loss: 2.2692513465881348\n",
      "  batch 8 loss: 2.2566847801208496\n",
      "  batch 9 loss: 2.033998727798462\n",
      "  batch 10 loss: 2.1945385932922363\n",
      "  batch 11 loss: 2.4030494689941406\n",
      "  batch 12 loss: 2.3953418731689453\n",
      "  batch 13 loss: 2.097421646118164\n",
      "  batch 14 loss: 2.245469093322754\n",
      "  batch 15 loss: 2.457960844039917\n",
      "  batch 16 loss: 2.188920021057129\n",
      "  batch 17 loss: 2.0824215412139893\n",
      "  batch 18 loss: 2.265122413635254\n",
      "  batch 19 loss: 2.0650722980499268\n",
      "  batch 20 loss: 2.171532154083252\n",
      "  batch 21 loss: 2.138645648956299\n",
      "  batch 22 loss: 2.330091714859009\n",
      "  batch 23 loss: 2.3881285190582275\n",
      "  batch 24 loss: 2.2815046310424805\n",
      "  batch 25 loss: 2.3945865631103516\n",
      "  batch 26 loss: 2.4818897247314453\n",
      "  batch 27 loss: 2.199660539627075\n",
      "  batch 28 loss: 1.9150657653808594\n",
      "  batch 29 loss: 2.093184232711792\n",
      "  batch 30 loss: 2.4139583110809326\n",
      "  batch 31 loss: 2.2324769496917725\n",
      "  batch 32 loss: 2.3491504192352295\n",
      "  batch 33 loss: 2.3800010681152344\n",
      "  batch 34 loss: 2.0679733753204346\n",
      "  batch 35 loss: 2.1719279289245605\n",
      "  batch 36 loss: 1.9630299806594849\n",
      "  batch 37 loss: 1.9688224792480469\n",
      "  batch 38 loss: 2.2254490852355957\n",
      "  batch 39 loss: 2.212864398956299\n",
      "  batch 40 loss: 2.0074069499969482\n",
      "  batch 41 loss: 2.2734878063201904\n",
      "  batch 42 loss: 2.317335844039917\n",
      "  batch 43 loss: 1.8616042137145996\n",
      "  batch 44 loss: 2.22629976272583\n",
      "  batch 45 loss: 2.2179453372955322\n",
      "  batch 46 loss: 2.3252646923065186\n",
      "  batch 47 loss: 2.2052316665649414\n",
      "  batch 48 loss: 2.1921989917755127\n",
      "  batch 49 loss: 2.197066068649292\n",
      "  batch 50 loss: 2.1342363357543945\n",
      "  batch 51 loss: 2.436445951461792\n",
      "  batch 52 loss: 2.1286470890045166\n",
      "  batch 53 loss: 2.2353315353393555\n",
      "  batch 54 loss: 1.9328292608261108\n",
      "  batch 55 loss: 2.394745111465454\n",
      "  batch 56 loss: 2.1898624897003174\n",
      "  batch 57 loss: 2.1752331256866455\n",
      "  batch 58 loss: 2.1651785373687744\n",
      "  batch 59 loss: 1.9996496438980103\n",
      "  batch 60 loss: 2.3483712673187256\n",
      "  batch 61 loss: 2.3458445072174072\n",
      "  batch 62 loss: 2.2669677734375\n",
      "  batch 63 loss: 2.2122602462768555\n",
      "  batch 64 loss: 2.119490385055542\n",
      "  batch 65 loss: 2.109743356704712\n",
      "  batch 66 loss: 2.1725170612335205\n",
      "  batch 67 loss: 1.8831899166107178\n",
      "  batch 68 loss: 2.267163038253784\n",
      "  batch 69 loss: 2.0004236698150635\n",
      "  batch 70 loss: 2.3645434379577637\n",
      "  batch 71 loss: 2.0962271690368652\n",
      "  batch 72 loss: 2.312666893005371\n",
      "  batch 73 loss: 1.9307044744491577\n",
      "  batch 74 loss: 2.1258983612060547\n",
      "  batch 75 loss: 2.052016496658325\n",
      "  batch 76 loss: 2.1337552070617676\n",
      "  batch 77 loss: 2.4303622245788574\n",
      "  batch 78 loss: 2.2321999073028564\n",
      "  batch 79 loss: 2.291733503341675\n",
      "  batch 80 loss: 2.2877421379089355\n",
      "  batch 81 loss: 2.23370361328125\n",
      "  batch 82 loss: 2.0763320922851562\n",
      "  batch 83 loss: 2.2677550315856934\n",
      "  batch 84 loss: 2.0556774139404297\n",
      "  batch 85 loss: 1.9990521669387817\n",
      "  batch 86 loss: 2.261253833770752\n",
      "  batch 87 loss: 2.014272689819336\n",
      "  batch 88 loss: 2.0621423721313477\n",
      "  batch 89 loss: 2.383467674255371\n",
      "  batch 90 loss: 2.421504259109497\n",
      "  batch 91 loss: 2.3638150691986084\n",
      "  batch 92 loss: 2.1602094173431396\n",
      "  batch 93 loss: 2.224078893661499\n",
      "  batch 94 loss: 2.214138984680176\n",
      "  batch 95 loss: 2.10143780708313\n",
      "  batch 96 loss: 2.0091984272003174\n",
      "  batch 97 loss: 2.2685329914093018\n",
      "  batch 98 loss: 2.3730340003967285\n",
      "  batch 99 loss: 1.9376182556152344\n",
      "  batch 100 loss: 2.5352227687835693\n",
      "  batch 101 loss: 2.262439250946045\n",
      "  batch 102 loss: 2.01491641998291\n",
      "  batch 103 loss: 2.158914566040039\n",
      "  batch 104 loss: 2.0946691036224365\n",
      "  batch 105 loss: 2.0425209999084473\n",
      "  batch 106 loss: 2.356306314468384\n",
      "  batch 107 loss: 2.2639169692993164\n",
      "  batch 108 loss: 2.2367515563964844\n",
      "  batch 109 loss: 2.235729455947876\n",
      "  batch 110 loss: 2.2625339031219482\n",
      "  batch 111 loss: 2.1558258533477783\n",
      "  batch 112 loss: 2.0284292697906494\n",
      "  batch 113 loss: 2.0555131435394287\n",
      "  batch 114 loss: 2.2513322830200195\n",
      "  batch 115 loss: 2.2151293754577637\n",
      "  batch 116 loss: 2.878984212875366\n",
      "  batch 1 loss: 2.0939645767211914\n",
      "  batch 2 loss: 1.986749291419983\n",
      "  batch 3 loss: 2.123267412185669\n",
      "  batch 4 loss: 2.2244019508361816\n",
      "  batch 5 loss: 2.162766218185425\n",
      "  batch 6 loss: 2.5268707275390625\n",
      "  batch 7 loss: 2.281630754470825\n",
      "  batch 8 loss: 2.253077268600464\n",
      "  batch 9 loss: 2.0273704528808594\n",
      "  batch 10 loss: 2.187786817550659\n",
      "  batch 11 loss: 2.363999605178833\n",
      "  batch 12 loss: 2.387822151184082\n",
      "  batch 13 loss: 2.081350803375244\n",
      "  batch 14 loss: 2.2499473094940186\n",
      "  batch 15 loss: 2.44107723236084\n",
      "  batch 16 loss: 2.185767889022827\n",
      "  batch 17 loss: 2.0597872734069824\n",
      "  batch 18 loss: 2.246433973312378\n",
      "  batch 19 loss: 2.024419069290161\n",
      "  batch 20 loss: 2.1520535945892334\n",
      "  batch 21 loss: 2.146956205368042\n",
      "  batch 22 loss: 2.313516616821289\n",
      "  batch 23 loss: 2.3921098709106445\n",
      "  batch 24 loss: 2.251051902770996\n",
      "  batch 25 loss: 2.350609064102173\n",
      "  batch 26 loss: 2.4703800678253174\n",
      "  batch 27 loss: 2.1693153381347656\n",
      "  batch 28 loss: 1.8854899406433105\n",
      "  batch 29 loss: 2.0656166076660156\n",
      "  batch 30 loss: 2.3924546241760254\n",
      "  batch 31 loss: 2.2185022830963135\n",
      "  batch 32 loss: 2.3297159671783447\n",
      "  batch 33 loss: 2.310690402984619\n",
      "  batch 34 loss: 2.032426595687866\n",
      "  batch 35 loss: 2.140533924102783\n",
      "  batch 36 loss: 1.9357414245605469\n",
      "  batch 37 loss: 1.9478946924209595\n",
      "  batch 38 loss: 2.1764752864837646\n",
      "  batch 39 loss: 2.19036865234375\n",
      "  batch 40 loss: 1.9972070455551147\n",
      "  batch 41 loss: 2.2320876121520996\n",
      "  batch 42 loss: 2.274019241333008\n",
      "  batch 43 loss: 1.841109275817871\n",
      "  batch 44 loss: 2.2282023429870605\n",
      "  batch 45 loss: 2.2012743949890137\n",
      "  batch 46 loss: 2.2860817909240723\n",
      "  batch 47 loss: 2.187580108642578\n",
      "  batch 48 loss: 2.1801488399505615\n",
      "  batch 49 loss: 2.176478147506714\n",
      "  batch 50 loss: 2.0922255516052246\n",
      "  batch 51 loss: 2.4068634510040283\n",
      "  batch 52 loss: 2.126476287841797\n",
      "  batch 53 loss: 2.1927032470703125\n",
      "  batch 54 loss: 1.9166218042373657\n",
      "  batch 55 loss: 2.374746084213257\n",
      "  batch 56 loss: 2.1494879722595215\n",
      "  batch 57 loss: 2.177690267562866\n",
      "  batch 58 loss: 2.161184072494507\n",
      "  batch 59 loss: 1.963930368423462\n",
      "  batch 60 loss: 2.3347790241241455\n",
      "  batch 61 loss: 2.3347253799438477\n",
      "  batch 62 loss: 2.25205135345459\n",
      "  batch 63 loss: 2.188892126083374\n",
      "  batch 64 loss: 2.091437578201294\n",
      "  batch 65 loss: 2.112812042236328\n",
      "  batch 66 loss: 2.1443309783935547\n",
      "  batch 67 loss: 1.8569022417068481\n",
      "  batch 68 loss: 2.2261509895324707\n",
      "  batch 69 loss: 1.9684216976165771\n",
      "  batch 70 loss: 2.3303163051605225\n",
      "  batch 71 loss: 2.0952634811401367\n",
      "  batch 72 loss: 2.264921188354492\n",
      "  batch 73 loss: 1.8988310098648071\n",
      "  batch 74 loss: 2.118159770965576\n",
      "  batch 75 loss: 2.06229567527771\n",
      "  batch 76 loss: 2.1162052154541016\n",
      "  batch 77 loss: 2.3979198932647705\n",
      "  batch 78 loss: 2.2135708332061768\n",
      "  batch 79 loss: 2.302816390991211\n",
      "  batch 80 loss: 2.2913553714752197\n",
      "  batch 81 loss: 2.257312059402466\n",
      "  batch 82 loss: 2.0629897117614746\n",
      "  batch 83 loss: 2.2293460369110107\n",
      "  batch 84 loss: 2.0620388984680176\n",
      "  batch 85 loss: 1.9918259382247925\n",
      "  batch 86 loss: 2.252157688140869\n",
      "  batch 87 loss: 1.977305293083191\n",
      "  batch 88 loss: 2.024930953979492\n",
      "  batch 89 loss: 2.338737726211548\n",
      "  batch 90 loss: 2.4146406650543213\n",
      "  batch 91 loss: 2.3690035343170166\n",
      "  batch 92 loss: 2.1840686798095703\n",
      "  batch 93 loss: 2.1811370849609375\n",
      "  batch 94 loss: 2.190243721008301\n",
      "  batch 95 loss: 2.111067295074463\n",
      "  batch 96 loss: 2.0159554481506348\n",
      "  batch 97 loss: 2.2436132431030273\n",
      "  batch 98 loss: 2.3704421520233154\n",
      "  batch 99 loss: 1.901094913482666\n",
      "  batch 100 loss: 2.5309104919433594\n",
      "  batch 101 loss: 2.266464948654175\n",
      "  batch 102 loss: 1.9768548011779785\n",
      "  batch 103 loss: 2.1367900371551514\n",
      "  batch 104 loss: 2.052483320236206\n",
      "  batch 105 loss: 2.0403690338134766\n",
      "  batch 106 loss: 2.325948715209961\n",
      "  batch 107 loss: 2.2658467292785645\n",
      "  batch 108 loss: 2.214989185333252\n",
      "  batch 109 loss: 2.220775604248047\n",
      "  batch 110 loss: 2.2538399696350098\n",
      "  batch 111 loss: 2.115910768508911\n",
      "  batch 112 loss: 1.9900240898132324\n",
      "  batch 113 loss: 2.04416561126709\n",
      "  batch 114 loss: 2.236682176589966\n",
      "  batch 115 loss: 2.2171831130981445\n",
      "  batch 116 loss: 2.8972816467285156\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model = BIGRU(30, 50, num_class=32, device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "for i in range(10):\n",
    "    loss = pytorch_utils.train_one_epoch(i, model, pytorch_utils.make_dataset(X_train, y_train, batch_size=64), optimizer, loss_fn, device=device)\n",
    "# torch.save(model, 'bigru.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(torch.Tensor(X_test).to(device))\n",
    "    maxk = 1\n",
    "    _, y_pred = output.topk(maxk, 1, True, True)\n",
    "    y_pred = y_pred.t()[0].cpu().numpy()\n",
    "\n",
    "# y_pred = model(X_test)\n",
    "# y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.00      0.00      0.00       138\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.72      0.18      0.29        71\n",
      "           4       0.00      0.00      0.00        28\n",
      "           5       0.00      0.00      0.00        14\n",
      "           6       0.00      0.00      0.00        17\n",
      "           7       0.34      0.49      0.40       223\n",
      "           8       0.00      0.00      0.00        22\n",
      "           9       0.00      0.00      0.00         2\n",
      "          10       0.21      0.33      0.25       101\n",
      "          11       0.00      0.00      0.00        49\n",
      "          12       0.00      0.00      0.00        23\n",
      "          13       0.00      0.00      0.00        17\n",
      "          14       0.00      0.00      0.00        10\n",
      "          15       0.00      0.00      0.00        10\n",
      "          16       0.35      0.47      0.40       101\n",
      "          17       0.00      0.00      0.00        20\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.35      0.25      0.29       319\n",
      "          20       0.00      0.00      0.00        57\n",
      "          21       0.17      0.03      0.05       114\n",
      "          22       0.25      0.19      0.21       195\n",
      "          23       0.21      0.19      0.20       226\n",
      "          24       0.00      0.00      0.00         2\n",
      "          25       1.00      0.02      0.04        88\n",
      "          26       0.00      0.00      0.00        38\n",
      "          27       0.00      0.00      0.00        19\n",
      "          28       0.00      0.00      0.00       403\n",
      "          30       0.36      0.70      0.48       720\n",
      "          31       0.28      0.49      0.36       583\n",
      "\n",
      "    accuracy                           0.32      3630\n",
      "   macro avg       0.14      0.11      0.10      3630\n",
      "weighted avg       0.25      0.32      0.25      3630\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/aleksandr_algasov/env/gestures/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/aleksandr_algasov/env/gestures/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/aleksandr_algasov/env/gestures/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f4a543a23d0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, include_values=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
